import requests
import time
import random
from bs4 import BeautifulSoup
import json
from urllib.parse import urlencode, quote
from fake_useragent import UserAgent
import itertools
from dataclasses import dataclass
from typing import List, Dict, Optional
import threading
from concurrent.futures import ThreadPoolExecutor
import logging
import csv
from datetime import datetime, time as dt_time
import os

# Th√™m Selenium imports
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from webdriver_manager.chrome import ChromeDriverManager

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class NewsArticle:
    headline: str
    link: str
    date: str
    source: str = ""

@dataclass
class CrawlStats:
    total_keywords: int = 0
    total_articles: int = 0
    captcha_skipped: int = 0
    errors: int = 0
    successful_requests: int = 0
    failed_requests: int = 0

class GoogleNewsScraper:
    def __init__(self, proxy_list: List[str] = None, use_selenium: bool = False):
        """
        Initialize scraper with anti-CAPTCHA measures
        
        Args:
            proxy_list: List of proxy servers in format 'ip:port' or 'username:password@ip:port'
            use_selenium: Whether to use Selenium for bypassing CAPTCHA
        """
        self.ua = UserAgent()
        self.proxy_list = proxy_list or []
        self.proxy_cycle = itertools.cycle(self.proxy_list) if self.proxy_list else None
        self.session_pool = {}
        self.request_count = 0
        self.last_request_time = 0
        self.use_selenium = use_selenium
        self.driver = None
        
        # Rate limiting settings
        self.min_delay = 2  # Minimum delay between requests
        self.max_delay = 5  # Maximum delay between requests
        self.requests_per_session = 10  # Switch session after N requests
        
        # Kh·ªüi t·∫°o Selenium driver n·∫øu c·∫ßn
        if self.use_selenium:
            self._setup_selenium_driver()
        
    def _setup_selenium_driver(self):
        """Thi·∫øt l·∫≠p Selenium WebDriver v·ªõi c√°c options t·ªëi ∆∞u"""
        try:
            chrome_options = Options()
            
            # C√°c options ƒë·ªÉ tƒÉng t·ªëc v√† ·∫©n browser
            chrome_options.add_argument('--headless')  # Ch·∫°y ·∫©n browser
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--disable-blink-features=AutomationControlled')
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            
            # User agent ng·∫´u nhi√™n
            chrome_options.add_argument(f'--user-agent={self.ua.random}')
            
            # Proxy n·∫øu c√≥
            if self.proxy_cycle:
                proxy = next(self.proxy_cycle)
                if '@' not in proxy:  # Simple proxy
                    chrome_options.add_argument(f'--proxy-server=http://{proxy}')
            
            # Kh·ªüi t·∫°o driver v·ªõi auto-detection architecture
            try:
                # Th·ª≠ download driver ph√π h·ª£p v·ªõi OS
                service = Service(ChromeDriverManager().install())
                self.driver = webdriver.Chrome(service=service, options=chrome_options)
            except Exception as e1:
                logger.warning(f"L·ªói ChromeDriverManager: {e1}")
                # Fallback: th·ª≠ d√πng chromedriver trong PATH
                try:
                    self.driver = webdriver.Chrome(options=chrome_options)
                except Exception as e2:
                    logger.error(f"L·ªói Chrome trong PATH: {e2}")
                    raise Exception("Kh√¥ng th·ªÉ kh·ªüi t·∫°o Chrome WebDriver")
            
            # Script ƒë·ªÉ ·∫©n automation detection
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            logger.info("‚úÖ Selenium WebDriver ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o th√†nh c√¥ng")
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói kh·ªüi t·∫°o Selenium: {str(e)}")
            logger.info("üí° S·∫Ω s·ª≠ d·ª•ng RSS mode thay th·∫ø")
            self.use_selenium = False
    
    def _get_real_urls_with_selenium(self, keyword: str, max_results: int = 20) -> List[NewsArticle]:
        """
        S·ª≠ d·ª•ng Selenium ƒë·ªÉ l·∫•y current URL th·ª±c t·∫ø t·ª´ Google News
        
        Args:
            keyword: T·ª´ kh√≥a t√¨m ki·∫øm
            max_results: S·ªë b√†i t·ªëi ƒëa
            
        Returns:
            List c√°c NewsArticle v·ªõi URL th·ª±c t·∫ø
        """
        articles = []
        
        if not self.driver:
            logger.error("‚ùå Selenium driver ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o")
            return articles
        
        try:
            # Build Google News search URL
            search_url = f"https://news.google.com/search?q={quote(keyword)}&hl=en&gl=us"
            
            logger.info(f"üîç Selenium ƒëang t√¨m ki·∫øm: {keyword}")
            
            # M·ªü trang t√¨m ki·∫øm
            self.driver.get(search_url)
            
            # ƒê·ª£i trang load
            WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "article"))
            )
            
            # T√¨m t·∫•t c·∫£ c√°c b√†i b√°o
            article_elements = self.driver.find_elements(By.TAG_NAME, "article")
            
            logger.info(f"üì∞ T√¨m th·∫•y {len(article_elements)} b√†i b√°o tr√™n trang")
            
            for i, article_elem in enumerate(article_elements[:max_results]):
                try:
                    # T√¨m link trong article
                    link_elem = article_elem.find_element(By.TAG_NAME, "a")
                    
                    if link_elem:
                        # Click v√†o link ƒë·ªÉ l·∫•y current URL
                        original_url = link_elem.get_attribute('href')
                        
                        # M·ªü tab m·ªõi
                        self.driver.execute_script("window.open('');")
                        self.driver.switch_to.window(self.driver.window_handles[1])
                        
                        # Navigate ƒë·∫øn link
                        self.driver.get(original_url)
                        
                        # ƒê·ª£i m·ªôt ch√∫t ƒë·ªÉ redirect ho√†n th√†nh
                        time.sleep(2)
                        
                        # L·∫•y current URL (ƒë√¢y l√† URL th·ª±c t·∫ø sau khi redirect)
                        real_url = self.driver.current_url
                        
                        # L·∫•y title t·ª´ trang th·ª±c t·∫ø
                        try:
                            headline = self.driver.title
                        except:
                            headline = f"Article {i+1}"
                        
                        # ƒê√≥ng tab hi·ªán t·∫°i v√† quay v·ªÅ tab ch√≠nh
                        self.driver.close()
                        self.driver.switch_to.window(self.driver.window_handles[0])
                        
                        # T·∫°o NewsArticle v·ªõi URL th·ª±c t·∫ø
                        article = NewsArticle(
                            headline=headline,
                            link=real_url,
                            date="Recent",
                            source=self._extract_domain_from_url(real_url)
                        )
                        articles.append(article)
                        
                        logger.info(f"‚úÖ [{i+1}] {headline[:60]}...")
                        logger.info(f"üîó Real URL: {real_url}")
                        
                        # Delay ng·∫Øn gi·ªØa c√°c b√†i
                        time.sleep(random.uniform(1, 2))
                        
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è L·ªói x·ª≠ l√Ω b√†i {i+1}: {str(e)}")
                    continue
            
            logger.info(f"üéØ Ho√†n th√†nh: {len(articles)} URLs th·ª±c t·∫ø cho '{keyword}'")
            
        except TimeoutException:
            logger.error("‚ùå Timeout khi t·∫£i trang Google News")
        except Exception as e:
            logger.error(f"‚ùå L·ªói Selenium: {str(e)}")
        
        return articles
    
    def _extract_domain_from_url(self, url: str) -> str:
        """Tr√≠ch xu·∫•t domain t·ª´ URL"""
        try:
            from urllib.parse import urlparse
            parsed = urlparse(url)
            domain = parsed.netloc
            if domain.startswith('www.'):
                domain = domain[4:]
            return domain
        except:
            return "N/A"
    
    def search_google_news_with_selenium(self, keyword: str, max_results: int = 20) -> List[NewsArticle]:
        """
        Method m·ªõi ƒë·ªÉ t√¨m ki·∫øm Google News v·ªõi Selenium
        
        Args:
            keyword: T·ª´ kh√≥a t√¨m ki·∫øm
            max_results: S·ªë b√†i t·ªëi ƒëa
            
        Returns:
            List NewsArticle v·ªõi URLs th·ª±c t·∫ø
        """
        if self.use_selenium and self.driver:
            return self._get_real_urls_with_selenium(keyword, max_results)
        else:
            # Fallback v·ªÅ method c≈©
            return self.search_google_news(keyword, max_results)
    
    def close_selenium(self):
        """ƒê√≥ng Selenium driver"""
        if self.driver:
            try:
                self.driver.quit()
                logger.info("‚úÖ Selenium driver ƒë√£ ƒë∆∞·ª£c ƒë√≥ng")
            except Exception as e:
                logger.error(f"‚ùå L·ªói ƒë√≥ng Selenium: {str(e)}")
    
    def get_rotating_session(self) -> requests.Session:
        """Get a session with rotating proxy and headers"""
        session_id = self.request_count // self.requests_per_session
        
        if session_id not in self.session_pool:
            session = requests.Session()
            
            # Set rotating proxy
            if self.proxy_cycle:
                proxy = next(self.proxy_cycle)
                if '@' in proxy:
                    # Authenticated proxy
                    auth, server = proxy.split('@')
                    username, password = auth.split(':')
                    session.auth = (username, password)
                    session.proxies = {
                        'http': f'http://{server}',
                        'https': f'https://{server}'
                    }
                else:
                    # Simple proxy
                    session.proxies = {
                        'http': f'http://{proxy}',
                        'https': f'https://{proxy}'
                    }
            
            # Set realistic headers
            session.headers.update(self._get_realistic_headers())
            
            self.session_pool[session_id] = session
            
        return self.session_pool[session_id]
    
    def _get_realistic_headers(self) -> Dict[str, str]:
        """Generate realistic browser headers"""
        return {
            'User-Agent': self.ua.random,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9,vi;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0',
        }
    
    def _smart_delay(self):
        """Implement smart delay to avoid rate limiting"""
        current_time = time.time()
        elapsed = current_time - self.last_request_time
        
        # Base delay + random jitter
        delay = random.uniform(self.min_delay, self.max_delay)
        
        # Add extra delay if requests are too frequent
        if elapsed < 1:
            delay += random.uniform(2, 4)
        
        # Occasional longer pause to appear more human-like
        if random.random() < 0.1:  # 10% chance
            delay += random.uniform(10, 20)
            
        logger.info(f"Waiting {delay:.2f} seconds...")
        time.sleep(delay)
        self.last_request_time = time.time()
    
    def _crawl_via_crawlbase(self, url: str) -> str:
        """Crawl trang web s·ª≠ d·ª•ng Crawlbase API v·ªõi normal token"""
        try:
            # S·ª≠ d·ª•ng normal token
            normal_token = "xKfElFt5FvM613yeR4Pz8A"
            api_url = f"https://api.crawlbase.com/?token={normal_token}&url={url}"
            
            # Th√™m headers c·∫ßn thi·∫øt
            headers = {
                'Accept-Encoding': 'gzip',
                'User-Agent': self.ua.random,
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.9',
            }
            
            logger.info(f"Calling Crawlbase API for URL: {url}")
            response = requests.get(api_url, headers=headers, timeout=90)
            
            # Log response status v√† headers
            logger.info(f"Crawlbase API Response Status: {response.status_code}")
            logger.info("Crawlbase Response Headers:")
            for key, value in response.headers.items():
                logger.info(f"{key}: {value}")
            
            response.raise_for_status()
            
            # X·ª≠ l√Ω response content
            if 'gzip' in response.headers.get('Content-Encoding', '').lower():
                import gzip
                content = gzip.decompress(response.content)
                html = content.decode('utf-8')
            else:
                html = response.text
            
            # In ra 500 k√Ω t·ª± ƒë·∫ßu c·ªßa HTML ƒë·ªÉ ki·ªÉm tra
            logger.info("First 500 characters of response:")
            logger.info(html[:500])
            
            return html
            
        except Exception as e:
            logger.error(f"Crawlbase API error: {str(e)}")
            return ""

    def search_google_news(self, keyword: str, max_results: int = 20) -> List[NewsArticle]:
        """
        Search Google News for a specific keyword
        
        Args:
            keyword: Search term
            max_results: Maximum number of results to return
            
        Returns:
            List of NewsArticle objects
        """
        articles = []
        
        try:
            # Build search URL
            params = {
                'q': keyword,
                'tbm': 'nws',  # News search
                'num': min(max_results, 100),  # Google limits to 100 per page
                'hl': 'en',
                'gl': 'us'
            }
            
            url = f"https://www.google.com/search?{urlencode(params)}"
            
            # Apply smart delay
            self._smart_delay()
            
            # Get session with rotation
            session = self.get_rotating_session()
            
            logger.info(f"Searching for: {keyword}")
            logger.info(f"Request #{self.request_count + 1}")
            
            # Make request
            response = session.get(url, timeout=30)
            response.raise_for_status()
            
            self.request_count += 1
            
            # Check for CAPTCHA
            if self._is_captcha_page(response.text):
                logger.warning("CAPTCHA detected! Switching to Crawlbase API...")
                html = self._crawl_via_crawlbase(url)
                if html:
                    articles = self._parse_google_news_results(html)
                else:
                    logger.warning("Crawlbase API failed, falling back to RSS...")
                    return self._handle_captcha_fallback(keyword, max_results)
            else:
                # Parse results
                articles = self._parse_google_news_results(response.text)
            
            logger.info(f"Found {len(articles)} articles for '{keyword}'")
            
        except Exception as e:
            logger.error(f"Error searching for '{keyword}': {str(e)}")
            
        return articles[:max_results]
    
    def _is_captcha_page(self, html: str) -> bool:
        """Check if the response contains a CAPTCHA"""
        captcha_indicators = [
            'captcha',
            'unusual traffic',
            'automated queries',
            'recaptcha',
            'verify you are human'
        ]
        
        html_lower = html.lower()
        return any(indicator in html_lower for indicator in captcha_indicators)
    
    def _handle_captcha_fallback(self, keyword: str, max_results: int) -> List[NewsArticle]:
        """Fallback strategy when CAPTCHA is encountered"""
        logger.info("Implementing CAPTCHA fallback strategy...")
        
        # Strategy 1: Switch to a different proxy/session
        if self.proxy_list:
            logger.info("Switching to next proxy...")
            # Force new session
            session_id = self.request_count // self.requests_per_session
            if session_id in self.session_pool:
                del self.session_pool[session_id]
        
        # Strategy 2: Extended delay
        delay = random.uniform(60, 120)  # 1-2 minutes
        logger.info(f"Extended delay: {delay:.2f} seconds")
        time.sleep(delay)
        
        # Strategy 3: Try alternative approach (RSS feeds)
        return self._search_via_rss(keyword, max_results)
    
    def _search_via_rss(self, keyword: str, max_results: int) -> List[NewsArticle]:
        """Alternative search using Google News RSS"""
        try:
            # Google News RSS URL
            rss_url = f"https://news.google.com/rss/search?q={urlencode({'': keyword})[1:]}&hl=en&gl=us"
            
            session = self.get_rotating_session()
            response = session.get(rss_url, timeout=30)
            response.raise_for_status()
            
            # In ra 500 k√Ω t·ª± ƒë·∫ßu c·ªßa RSS response ƒë·ªÉ debug
            logger.warning("In ra 500 k√Ω t·ª± ƒë·∫ßu c·ªßa RSS response ƒë·ªÉ ki·ªÉm tra...")
            print("\n===== RSS RESPONSE (500 k√Ω t·ª± ƒë·∫ßu) =====\n")
            print(response.text[:500])
            print("\n========================================\n")
            
            # Parse RSS feed
            soup = BeautifulSoup(response.content, 'xml')
            articles = []
            
            for item in soup.find_all('item')[:max_results]:
                try:
                    article = NewsArticle(
                        headline=item.title.text if item.title else "N/A",
                        link=item.link.text if item.link else "N/A",
                        date=item.pubDate.text if item.pubDate else "N/A",
                        source=item.source.text if item.source else "N/A"
                    )
                    articles.append(article)
                except Exception as e:
                    logger.warning(f"Error parsing RSS item: {e}")
                    continue
                    
            logger.info(f"RSS fallback found {len(articles)} articles")
            return articles
            
        except Exception as e:
            logger.error(f"RSS fallback failed: {e}")
            return []
    
    def _parse_google_news_results(self, html: str) -> List[NewsArticle]:
        """Parse Google News search results"""
        soup = BeautifulSoup(html, 'html.parser')
        articles = []
        
        # Google News result selectors (these may need updates)
        selectors = [
            'div[data-ved] a[href*="/url?"]',  # Standard news results
            'div.g a h3',  # Alternative selector
            'div.SoaBEf a',  # Another common selector
        ]
        
        for selector in selectors:
            elements = soup.select(selector)
            if elements:
                break
        else:
            # N·∫øu kh√¥ng t√¨m th·∫•y elements n√†o, in ra m·ªôt ph·∫ßn n·ªôi dung HTML ƒë·ªÉ debug
            logger.warning("Kh√¥ng t√¨m th·∫•y elements ph√π h·ª£p! In ra 500 k√Ω t·ª± ƒë·∫ßu c·ªßa HTML ƒë·ªÉ ki·ªÉm tra...")
            print("\n===== HTML RESPONSE (500 k√Ω t·ª± ƒë·∫ßu) =====\n")
            print(html[:500])
            print("\n========================================\n")
            return []
        
        for element in elements:
            try:
                # Extract headline
                headline_elem = element.find('h3') or element
                headline = headline_elem.get_text(strip=True) if headline_elem else "N/A"
                
                # Extract link
                link = element.get('href', '')
                if link.startswith('/url?q='):
                    # Decode Google redirect URL
                    link = link.split('/url?q=')[1].split('&')[0]
                
                # Extract date (this is tricky with Google's structure)
                date_elem = element.find_parent().find(text=lambda x: x and any(
                    term in x.lower() for term in ['ago', 'hours', 'days', 'minutes']
                ))
                date = date_elem.strip() if date_elem else "N/A"
                
                if headline != "N/A" and link:
                    article = NewsArticle(
                        headline=headline,
                        link=link,
                        date=date
                    )
                    articles.append(article)
                    
            except Exception as e:
                logger.warning(f"Error parsing article: {e}")
                continue
                
        return articles
    
    def scrape_multiple_keywords(self, keywords: List[str], max_results_per_keyword: int = 20) -> Dict[str, List[NewsArticle]]:
        """
        Scrape Google News for multiple keywords
        
        Args:
            keywords: List of search terms
            max_results_per_keyword: Maximum results per keyword
            
        Returns:
            Dictionary mapping keywords to their articles
        """
        results = {}
        
        logger.info(f"Starting scraping for {len(keywords)} keywords...")
        
        for i, keyword in enumerate(keywords, 1):
            logger.info(f"Processing keyword {i}/{len(keywords)}: {keyword}")
            
            articles = self.search_google_news(keyword, max_results_per_keyword)
            results[keyword] = articles
            
            # Progress update
            if i % 10 == 0:
                logger.info(f"Completed {i}/{len(keywords)} keywords")
        
        logger.info("Scraping completed!")
        return results
    
    def export_results(self, results: Dict[str, List[NewsArticle]], filename: str = "google_news_results.json"):
        """Export results to JSON file"""
        export_data = {}
        
        for keyword, articles in results.items():
            export_data[keyword] = [
                {
                    'headline': article.headline,
                    'link': article.link,
                    'date': article.date,
                    'source': article.source
                }
                for article in articles
            ]
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"Results exported to {filename}")

def generate_keyword_variations(base_keyword: str, max_pages: int = 10) -> List[str]:
    """
    Tr·∫£ v·ªÅ keyword g·ªëc - kh√¥ng t·∫°o variations, ch·ªâ l·∫•y b√†i m·ªõi nh·∫•t
    
    Args:
        base_keyword: Keyword g·ªëc
        max_pages: Kh√¥ng s·ª≠ d·ª•ng, ch·ªâ gi·ªØ ƒë·ªÉ compatibility
    
    Returns:
        List ch·ªâ ch·ª©a keyword g·ªëc
    """
    # Ch·ªâ tr·∫£ v·ªÅ keyword g·ªëc, kh√¥ng t·∫°o variations
    return [base_keyword]

def crawl_keyword_multiple_pages(keyword: str, pages_per_keyword: int = 10, articles_per_page: int = 20, stats: CrawlStats = None) -> List[NewsArticle]:
    """
    Crawl m·ªôt keyword - ch·ªâ l·∫•y b√†i m·ªõi nh·∫•t kh√¥ng d√πng variations
    
    Args:
        keyword: Keyword g·ªëc
        pages_per_keyword: Kh√¥ng s·ª≠ d·ª•ng, ch·ªâ gi·ªØ ƒë·ªÉ compatibility
        articles_per_page: S·ªë b√†i mu·ªën l·∫•y
        stats: Object th·ªëng k√™
    
    Returns:
        List b√†i b√°o m·ªõi nh·∫•t cho keyword
    """
    if stats is None:
        stats = CrawlStats()
    
    print(f"üîç Crawling latest articles for keyword: {keyword}")
    
    # Ch·ªâ crawl keyword g·ªëc, l·∫•y nhi·ªÅu b√†i h∆°n ƒë·ªÉ ƒë·∫£m b·∫£o c√≥ ƒë·ªß b√†i m·ªõi nh·∫•t
    articles = simple_crawl_rss(keyword, articles_per_page, stats)
    
    print(f"üéØ Found {len(articles)} articles for '{keyword}'")
    return articles

def simple_crawl_rss(keyword: str, max_results: int = 20, stats: CrawlStats = None) -> List[NewsArticle]:
    """Crawl Google News RSS ƒë∆°n gi·∫£n - √≠t b·ªã ch·∫∑n"""
    if stats is None:
        stats = CrawlStats()
    
    try:
        # Google News RSS URL v·ªõi keyword
        encoded_keyword = quote(keyword)
        rss_url = f"https://news.google.com/rss/search?q={encoded_keyword}&hl=en&gl=us&ceid=US:en"
        
        # Headers ƒë∆°n gi·∫£n
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        # G·ªçi RSS
        response = requests.get(rss_url, headers=headers, timeout=30)
        response.raise_for_status()
        
        # Parse RSS v·ªõi lxml ho·∫∑c html.parser l√†m fallback
        try:
            soup = BeautifulSoup(response.content, 'xml')
        except:
            try:
                soup = BeautifulSoup(response.content, 'lxml-xml')
            except:
                soup = BeautifulSoup(response.content, 'html.parser')
        
        articles = []
        
        # T√¨m t·∫•t c·∫£ c√°c item trong RSS
        items = soup.find_all('item')
        
        for i, item in enumerate(items[:max_results]):
            try:
                title = item.title.text if item.title else "N/A"
                link = item.link.text if item.link else "N/A"
                pub_date = item.pubDate.text if item.pubDate else "N/A"
                
                # L·∫•y source t·ª´ description ho·∫∑c source tag
                source = "N/A"
                if item.source:
                    source = item.source.text
                elif item.description:
                    desc = item.description.text
                    # Extract source t·ª´ description n·∫øu c√≥
                    import re
                    source_match = re.search(r'<a[^>]*>([^<]+)</a>', desc)
                    if source_match:
                        source = source_match.group(1)
                
                article = NewsArticle(
                    headline=title,
                    link=link,
                    date=pub_date,
                    source=source
                )
                articles.append(article)
                
            except Exception as e:
                stats.errors += 1
                continue
        
        stats.successful_requests += 1
        return articles
        
    except Exception as e:
        stats.failed_requests += 1
        stats.errors += 1
        return []

def simple_crawl_with_selenium(keyword: str, max_results: int = 20, stats: CrawlStats = None) -> List[NewsArticle]:
    """
    Crawl Google News s·ª≠ d·ª•ng Selenium ƒë·ªÉ l·∫•y current URL th·ª±c t·∫ø - bypass CAPTCHA
    
    Args:
        keyword: T·ª´ kh√≥a t√¨m ki·∫øm
        max_results: S·ªë b√†i t·ªëi ƒëa
        stats: Object th·ªëng k√™
        
    Returns:
        List NewsArticle v·ªõi URLs th·ª±c t·∫ø
    """
    if stats is None:
        stats = CrawlStats()
    
    try:
        logger.info(f"ü§ñ Kh·ªüi t·∫°o Selenium crawler cho: {keyword}")
        
        # T·∫°o scraper v·ªõi Selenium
        scraper = GoogleNewsScraper(use_selenium=True)
        
        if not scraper.use_selenium:
            logger.warning("‚ùå Selenium kh√¥ng kh·∫£ d·ª•ng, fallback v·ªÅ RSS")
            scraper.close_selenium()
            return simple_crawl_rss(keyword, max_results, stats)
        
        # Crawl v·ªõi Selenium
        articles = scraper.search_google_news_with_selenium(keyword, max_results)
        
        # ƒê√≥ng Selenium
        scraper.close_selenium()
        
        if articles:
            stats.successful_requests += 1
            logger.info(f"‚úÖ Selenium: T√¨m th·∫•y {len(articles)} URLs th·ª±c t·∫ø cho '{keyword}'")
        else:
            stats.failed_requests += 1
            logger.warning(f"‚ö†Ô∏è Selenium: Kh√¥ng t√¨m th·∫•y b√†i n√†o cho '{keyword}'")
        
        return articles
        
    except Exception as e:
        stats.failed_requests += 1
        stats.errors += 1
        logger.error(f"‚ùå L·ªói Selenium crawl cho '{keyword}': {str(e)}")
        return []

def smart_crawl_with_fallback(keyword: str, max_results: int = 20, stats: CrawlStats = None, use_selenium_first: bool = False) -> List[NewsArticle]:
    """
    Crawl th√¥ng minh v·ªõi fallback: th·ª≠ Selenium tr∆∞·ªõc, n·∫øu fail th√¨ d√πng RSS
    
    Args:
        keyword: T·ª´ kh√≥a t√¨m ki·∫øm
        max_results: S·ªë b√†i t·ªëi ƒëa
        stats: Object th·ªëng k√™
        use_selenium_first: C√≥ th·ª≠ Selenium tr∆∞·ªõc kh√¥ng
        
    Returns:
        List NewsArticle
    """
    if stats is None:
        stats = CrawlStats()
    
    articles = []
    
    # Th·ª≠ Selenium tr∆∞·ªõc n·∫øu ƒë∆∞·ª£c y√™u c·∫ßu
    if use_selenium_first:
        logger.info(f"ü§ñ Th·ª≠ Selenium tr∆∞·ªõc cho: {keyword}")
        articles = simple_crawl_with_selenium(keyword, max_results, stats)
        
        if articles and len(articles) >= max_results * 0.5:  # N·∫øu c√≥ √≠t nh·∫•t 50% s·ªë b√†i mong mu·ªën
            logger.info(f"‚úÖ Selenium th√†nh c√¥ng cho '{keyword}': {len(articles)} b√†i")
            return articles
        else:
            logger.warning(f"‚ö†Ô∏è Selenium kh√¥ng ƒë·∫°t k·ª≥ v·ªçng cho '{keyword}', fallback v·ªÅ RSS")
    
    # Fallback v·ªÅ RSS
    logger.info(f"üì° S·ª≠ d·ª•ng RSS fallback cho: {keyword}")
    rss_articles = simple_crawl_rss(keyword, max_results, stats)
    
    if rss_articles:
        logger.info(f"‚úÖ RSS th√†nh c√¥ng cho '{keyword}': {len(rss_articles)} b√†i")
        return rss_articles
    else:
        logger.warning(f"‚ùå C·∫£ Selenium v√† RSS ƒë·ªÅu th·∫•t b·∫°i cho '{keyword}'")
        return articles  # Tr·∫£ v·ªÅ k·∫øt qu·∫£ Selenium d√π √≠t

def crawl_keyword_multiple_pages_v2(keyword: str, pages_per_keyword: int = 10, articles_per_page: int = 20, stats: CrawlStats = None, use_selenium: bool = False) -> List[NewsArticle]:
    """
    Phi√™n b·∫£n m·ªõi c·ªßa crawl_keyword_multiple_pages v·ªõi t√πy ch·ªçn Selenium
    
    Args:
        keyword: Keyword g·ªëc
        pages_per_keyword: Kh√¥ng s·ª≠ d·ª•ng, ch·ªâ gi·ªØ ƒë·ªÉ compatibility
        articles_per_page: S·ªë b√†i mu·ªën l·∫•y
        stats: Object th·ªëng k√™
        use_selenium: C√≥ s·ª≠ d·ª•ng Selenium kh√¥ng
    
    Returns:
        List b√†i b√°o cho keyword
    """
    if stats is None:
        stats = CrawlStats()
    
    print(f"üîç Crawling keyword: {keyword}")
    
    if use_selenium:
        # D√πng smart crawl v·ªõi Selenium
        articles = smart_crawl_with_fallback(keyword, articles_per_page, stats, use_selenium_first=True)
    else:
        # D√πng RSS nh∆∞ c≈©
        articles = simple_crawl_rss(keyword, articles_per_page, stats)
    
    print(f"üéØ Found {len(articles)} articles for '{keyword}'")
    return articles

def extract_domain(url: str) -> str:
    """Tr√≠ch xu·∫•t domain t·ª´ URL"""
    try:
        if not url or url == 'N/A':
            return 'N/A'
        
        from urllib.parse import urlparse
        parsed = urlparse(url)
        domain = parsed.netloc
        
        # Lo·∫°i b·ªè 'www.' n·∫øu c√≥
        if domain.startswith('www.'):
            domain = domain[4:]
            
        return domain
    except:
        return 'N/A'

def bulk_crawl_to_csv(keywords: List[str], pages_per_keyword: int = 10, articles_per_page: int = 20, csv_filename: str = None) -> str:
    """
    Crawl nhi·ªÅu keyword v·ªõi nhi·ªÅu trang m·ªói keyword v√† xu·∫•t ra CSV
    
    Args:
        keywords: Danh s√°ch t·ª´ kh√≥a
        pages_per_keyword: S·ªë trang m·ªói keyword
        articles_per_page: S·ªë b√†i m·ªói trang
        csv_filename: T√™n file CSV output
    
    Returns:
        T√™n file CSV ƒë√£ t·∫°o
    """
    
    # T·∫°o t√™n file CSV v·ªõi timestamp
    if csv_filename is None:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        csv_filename = f"crawl_results_{timestamp}.csv"
    
    # Kh·ªüi t·∫°o stats
    stats = CrawlStats()
    stats.total_keywords = len(keywords)
    
    estimated_articles = len(keywords) * pages_per_keyword * articles_per_page
    
    print(f"üî• GOOGLE NEWS BULK CRAWLER - MULTI-PAGE MODE")
    print("=" * 70)
    print(f"üìù Keywords to crawl: {len(keywords)}")
    print(f"üìÑ Pages per keyword: {pages_per_keyword}")
    print(f"üìä Articles per page: {articles_per_page}")
    print(f"üéØ Estimated total articles: {estimated_articles}")
    print(f"üìÅ Output file: {csv_filename}")
    print("=" * 70)
    
    # T·∫°o v√† m·ªü file CSV
    with open(csv_filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
        fieldnames = [
            'keyword', 'article_number', 'headline', 'link', 
            'date', 'source', 'domain', 'crawl_timestamp'
        ]
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        
        # Crawl t·ª´ng keyword
        for i, keyword in enumerate(keywords, 1):
            print(f"\nüìã Processing keyword {i}/{len(keywords)}: {keyword}")
            
            # Delay ng·∫´u nhi√™n gi·ªØa c√°c keyword
            if i > 1:
                delay = random.uniform(5, 10)
                print(f"‚è≥ Waiting {delay:.1f} seconds between keywords...")
                time.sleep(delay)
            
            # Crawl nhi·ªÅu trang cho keyword n√†y
            all_articles = crawl_keyword_multiple_pages_v2(
                keyword, 
                pages_per_keyword, 
                articles_per_page, 
                stats, 
                use_selenium=True
            )
            
            # Ghi t·ª´ng b√†i b√°o v√†o CSV
            for j, article in enumerate(all_articles, 1):
                row = {
                    'keyword': keyword,
                    'article_number': j,
                    'headline': article.headline,
                    'link': article.link,
                    'date': article.date,
                    'source': article.source,
                    'domain': extract_domain(article.link),
                    'crawl_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }
                writer.writerow(row)
            
            stats.total_articles += len(all_articles)
            
            # Progress update
            print(f"üìä Completed {i}/{len(keywords)} keywords. Total articles so far: {stats.total_articles}")
    
    # In th·ªëng k√™ cu·ªëi c√πng
    print_final_stats(stats, csv_filename)
    
    return csv_filename

def print_final_stats(stats: CrawlStats, csv_filename: str):
    """In th·ªëng k√™ cu·ªëi c√πng"""
    print("\n" + "=" * 60)
    print("üéØ CRAWL STATISTICS")
    print("=" * 60)
    print(f"üìã Total Keywords: {stats.total_keywords}")
    print(f"üì∞ Total Articles: {stats.total_articles}")
    print(f"‚úÖ Successful Requests: {stats.successful_requests}")
    print(f"‚ùå Failed Requests: {stats.failed_requests}")
    print(f"‚ö†Ô∏è Total Errors: {stats.errors}")
    print(f"üö´ Captcha Skipped: {stats.captcha_skipped}")
    print(f"üìÅ Output File: {csv_filename}")
    
    if stats.total_keywords > 0:
        success_rate = (stats.successful_requests / stats.total_keywords) * 100
        avg_articles = stats.total_articles / stats.total_keywords if stats.total_keywords > 0 else 0
        print(f"üìà Success Rate: {success_rate:.1f}%")
        print(f"üìä Average Articles per Keyword: {avg_articles:.1f}")
    
    print("=" * 60)
    print("üéâ Crawl completed successfully!")

def crawl_single_keyword_deep(keyword: str, target_articles: int = 200, csv_filename: str = None) -> str:
    """
    Crawl 1 keyword ƒë·ªÉ l·∫•y b√†i m·ªõi nh·∫•t (kh√¥ng d√πng variations)
    
    Args:
        keyword: Keyword mu·ªën crawl
        target_articles: S·ªë b√†i m·ª•c ti√™u
        csv_filename: T√™n file CSV output
    
    Returns:
        T√™n file CSV ƒë√£ t·∫°o
    """
    
    # T·∫°o t√™n file CSV v·ªõi keyword v√† timestamp
    if csv_filename is None:
        safe_keyword = keyword.replace(' ', '_').replace('/', '_').replace('\\', '_')
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        csv_filename = f"{safe_keyword}_{timestamp}.csv"
    
    # Kh·ªüi t·∫°o stats
    stats = CrawlStats()
    stats.total_keywords = 1
    
    print(f"üî• CRAWL LATEST ARTICLES FOR SINGLE KEYWORD")
    print("=" * 60)
    print(f"üéØ Keyword: {keyword}")
    print(f"üìä Target articles: {target_articles}")
    print(f"üìÅ Output file: {csv_filename}")
    print("=" * 60)
    
    print(f"üîç Crawling latest articles for '{keyword}'...")
    
    # Ch·ªâ crawl keyword g·ªëc ƒë·ªÉ l·∫•y b√†i m·ªõi nh·∫•t
    articles = simple_crawl_rss(keyword, target_articles, stats)
    
    # T·∫°o v√† m·ªü file CSV
    with open(csv_filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
        fieldnames = [
            'article_number', 'headline', 'link', 
            'date', 'source', 'domain', 'crawl_timestamp'
        ]
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        
        # Ghi t·ª´ng b√†i b√°o v√†o CSV
        for i, article in enumerate(articles, 1):
            row = {
                'article_number': i,
                'headline': article.headline,
                'link': article.link,
                'date': article.date,
                'source': article.source,
                'domain': extract_domain(article.link),
                'crawl_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
            writer.writerow(row)
    
    stats.total_articles = len(articles)
    
    # In th·ªëng k√™ cu·ªëi c√πng
    print_single_keyword_stats(stats, csv_filename, keyword, len(articles), target_articles)
    
    return csv_filename

def print_single_keyword_stats(stats: CrawlStats, csv_filename: str, keyword: str, actual_articles: int, target_articles: int):
    """In th·ªëng k√™ cho single keyword crawl"""
    print("\n" + "=" * 60)
    print("üéØ CRAWL RESULTS")
    print("=" * 60)
    print(f"üîç Keyword: {keyword}")
    print(f"üì∞ Articles Found: {actual_articles}")
    print(f"üéØ Target Articles: {target_articles}")
    print(f"‚úÖ Successful Requests: {stats.successful_requests}")
    print(f"‚ùå Failed Requests: {stats.failed_requests}")
    print(f"‚ö†Ô∏è Total Errors: {stats.errors}")
    print(f"üö´ Captcha Skipped: {stats.captcha_skipped}")
    print(f"üìÅ Output File: {csv_filename}")
    
    # Progress percentage
    progress = min(actual_articles / target_articles * 100, 100) if target_articles > 0 else 0
    print(f"üìà Target Achievement: {progress:.1f}%")
    
    if actual_articles >= target_articles:
        print("üéâ Target achieved successfully!")
    else:
        print(f"‚ÑπÔ∏è Found {actual_articles} articles (target was {target_articles})")
    
    print("=" * 60)

def crawl_multiple_keywords_deep(keywords: List[str], target_articles_per_keyword: int = 200, csv_filename: str = None) -> str:
    """
    Crawl nhi·ªÅu keywords, m·ªói keyword l·∫•y ~200 b√†i
    
    Args:
        keywords: Danh s√°ch keywords
        target_articles_per_keyword: S·ªë b√†i m·ª•c ti√™u cho m·ªói keyword
        csv_filename: T√™n file CSV output
    
    Returns:
        T√™n file CSV ƒë√£ t·∫°o
    """
    
    # T·∫°o t√™n file CSV v·ªõi timestamp
    if csv_filename is None:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        csv_filename = f"multi_keywords_crawl_{timestamp}.csv"
    
    # Kh·ªüi t·∫°o stats t·ªïng
    total_stats = CrawlStats()
    total_stats.total_keywords = len(keywords)
    
    print(f"üî• MULTI-KEYWORDS DEEP CRAWLER")
    print("=" * 70)
    print(f"üìù Keywords: {len(keywords)}")
    print(f"üìä Target articles per keyword: {target_articles_per_keyword}")
    print(f"üéØ Total target articles: {len(keywords) * target_articles_per_keyword}")
    print(f"üìÅ Output file: {csv_filename}")
    print("=" * 70)
    
    # Hi·ªÉn th·ªã keywords s·∫Ω crawl
    print("üìã KEYWORDS TO CRAWL:")
    for i, keyword in enumerate(keywords, 1):
        print(f"   {i}. {keyword}")
    print()
    
    all_articles = []
    
    # T·∫°o v√† m·ªü file CSV
    with open(csv_filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
        fieldnames = [
            'keyword', 'article_number', 'headline', 'link', 
            'date', 'source', 'domain', 'crawl_timestamp'
        ]
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        
        # Crawl t·ª´ng keyword
        for keyword_index, keyword in enumerate(keywords, 1):
            print(f"\nüîç PROCESSING KEYWORD {keyword_index}/{len(keywords)}: {keyword}")
            print("-" * 50)
            
            # Delay gi·ªØa c√°c keywords
            if keyword_index > 1:
                delay = random.uniform(8, 15)
                print(f"‚è≥ Waiting {delay:.1f} seconds between keywords...")
                time.sleep(delay)
            
            # Crawl keyword n√†y
            keyword_articles = crawl_single_keyword_for_multi(
                keyword, 
                target_articles_per_keyword, 
                total_stats
            )
            
            # Ghi t·∫•t c·∫£ b√†i b√°o c·ªßa keyword n√†y v√†o CSV
            for i, article in enumerate(keyword_articles, 1):
                row = {
                    'keyword': keyword,
                    'article_number': i,
                    'headline': article.headline,
                    'link': article.link,
                    'date': article.date,
                    'source': article.source,
                    'domain': extract_domain(article.link),
                    'crawl_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }
                writer.writerow(row)
            
            all_articles.extend(keyword_articles)
            
            # Progress update
            print(f"‚úÖ Completed {keyword}: {len(keyword_articles)} articles")
            print(f"üìä Overall progress: {keyword_index}/{len(keywords)} keywords, {len(all_articles)} total articles")
    
    total_stats.total_articles = len(all_articles)
    
    # In th·ªëng k√™ cu·ªëi c√πng
    print_multi_keywords_stats(total_stats, csv_filename, keywords, target_articles_per_keyword, len(all_articles))
    
    return csv_filename

def crawl_single_keyword_for_multi(keyword: str, target_articles: int, stats: CrawlStats) -> List[NewsArticle]:
    """
    Crawl 1 keyword cho multi-keyword crawl - ch·ªâ l·∫•y b√†i m·ªõi nh·∫•t
    
    Args:
        keyword: Keyword mu·ªën crawl
        target_articles: S·ªë b√†i m·ª•c ti√™u
        stats: Object th·ªëng k√™ chung
    
    Returns:
        List c√°c b√†i b√°o m·ªõi nh·∫•t
    """
    
    print(f"üîç Crawling latest articles for '{keyword}'...")
    
    # Ch·ªâ crawl keyword g·ªëc v·ªõi s·ªë b√†i ƒë∆∞·ª£c y√™u c·∫ßu
    articles = simple_crawl_rss(keyword, target_articles, stats)
    
    print(f"üéØ Found {len(articles)} articles for '{keyword}'")
    return articles

def print_multi_keywords_stats(stats: CrawlStats, csv_filename: str, keywords: List[str], target_per_keyword: int, total_articles: int):
    """In th·ªëng k√™ cho multi-keywords crawl"""
    print("\n" + "=" * 70)
    print("üéØ MULTI-KEYWORDS CRAWL RESULTS")
    print("=" * 70)
    print(f"üìù Total Keywords: {len(keywords)}")
    print(f"üì∞ Total Articles Found: {total_articles}")
    print(f"üéØ Target Articles: {len(keywords) * target_per_keyword}")
    print(f"üìä Average Articles per Keyword: {total_articles / len(keywords):.1f}")
    print(f"‚úÖ Successful Requests: {stats.successful_requests}")
    print(f"‚ùå Failed Requests: {stats.failed_requests}")
    print(f"‚ö†Ô∏è Total Errors: {stats.errors}")
    print(f"üö´ Captcha Skipped: {stats.captcha_skipped}")
    print(f"üìÅ Output File: {csv_filename}")
    
    # Progress percentage
    target_total = len(keywords) * target_per_keyword
    progress = min(total_articles / target_total * 100, 100) if target_total > 0 else 0
    print(f"üìà Overall Achievement: {progress:.1f}%")
    
    if total_articles >= target_total:
        print("üéâ All targets achieved successfully!")
    else:
        print(f"‚ÑπÔ∏è Found {total_articles} articles (target was {target_total})")
    
    print("=" * 70)
    print("üéâ Multi-keywords crawl completed!")

def read_keywords_from_csv(csv_file_path: str, keyword_column: str = None, skip_header: bool = True) -> List[str]:
    """
    ƒê·ªçc keywords t·ª´ file CSV
    
    Args:
        csv_file_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file CSV ch·ª©a keywords
        keyword_column: T√™n c·ªôt ch·ª©a keywords (n·∫øu None s·∫Ω l·∫•y c·ªôt ƒë·∫ßu ti√™n)
        skip_header: C√≥ b·ªè qua header row kh√¥ng
    
    Returns:
        List c√°c keywords t·ª´ CSV
    """
    keywords = []
    
    try:
        print(f"üìñ Reading keywords from CSV file: {csv_file_path}")
        
        with open(csv_file_path, 'r', encoding='utf-8-sig') as csvfile:
            # Detect delimiter
            sample = csvfile.read(1024)
            csvfile.seek(0)
            
            delimiter = ','
            if ';' in sample and sample.count(';') > sample.count(','):
                delimiter = ';'
            elif '\t' in sample:
                delimiter = '\t'
            
            print(f"üìä Detected CSV delimiter: '{delimiter}'")
            
            reader = csv.reader(csvfile, delimiter=delimiter)
            
            # Skip header if needed
            if skip_header:
                try:
                    header = next(reader)
                    print(f"üìã CSV Header: {header}")
                    
                    # T·ª± ƒë·ªông detect keyword column n·∫øu kh√¥ng ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh
                    if keyword_column is None:
                        keyword_column = header[0]  # L·∫•y c·ªôt ƒë·∫ßu ti√™n
                        keyword_col_index = 0
                    else:
                        # T√¨m index c·ªßa keyword column
                        keyword_col_index = None
                        for i, col in enumerate(header):
                            if col.lower().strip() == keyword_column.lower().strip():
                                keyword_col_index = i
                                break
                        
                        if keyword_col_index is None:
                            print(f"‚ö†Ô∏è Column '{keyword_column}' not found. Using first column.")
                            keyword_col_index = 0
                    
                    print(f"üéØ Using column '{header[keyword_col_index]}' (index: {keyword_col_index}) for keywords")
                    
                except StopIteration:
                    print("‚ö†Ô∏è CSV file is empty")
                    return []
            else:
                keyword_col_index = 0
            
            # ƒê·ªçc keywords
            for row_num, row in enumerate(reader, start=2 if skip_header else 1):
                if row and len(row) > keyword_col_index:
                    keyword = row[keyword_col_index].strip()
                    if keyword:  # B·ªè qua d√≤ng tr·ªëng
                        keywords.append(keyword)
                        print(f"   ‚úÖ Row {row_num}: '{keyword}'")
                else:
                    print(f"   ‚ö†Ô∏è Row {row_num}: Empty or invalid row")
        
        print(f"üìä Total keywords loaded: {len(keywords)}")
        return keywords
        
    except FileNotFoundError:
        print(f"‚ùå File not found: {csv_file_path}")
        return []
    except Exception as e:
        print(f"‚ùå Error reading CSV file: {str(e)}")
        return []

def is_within_working_hours() -> bool:
    """
    Ki·ªÉm tra xem hi·ªán t·∫°i c√≥ trong gi·ªù l√†m vi·ªác kh√¥ng (10h - 18h)
    
    Returns:
        True n·∫øu trong gi·ªù l√†m vi·ªác, False n·∫øu kh√¥ng
    """
    now = datetime.now()
    current_time = now.time()
    
    # Gi·ªù l√†m vi·ªác: 10:00 - 18:00
    start_time = dt_time(10, 0)  # 10:00 AM
    end_time = dt_time(18, 0)    # 6:00 PM
    
    return start_time <= current_time <= end_time

def batch_crawl_keywords(keywords: List[str], articles_per_keyword: int = 10, batch_id: str = None) -> str:
    """
    Crawl keywords theo batch v·ªõi s·ªë l∆∞·ª£ng b√†i √≠t ƒë·ªÉ test t·∫ßn su·∫•t cao
    
    Args:
        keywords: Danh s√°ch keywords
        articles_per_keyword: S·ªë b√†i m·ªói keyword (m·∫∑c ƒë·ªãnh 10)
        batch_id: ID c·ªßa batch (n·∫øu None s·∫Ω t·ª± t·∫°o t·ª´ timestamp)
    
    Returns:
        T√™n file CSV ƒë√£ t·∫°o
    """
    
    # T·∫°o batch ID v√† t√™n file
    if batch_id is None:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        batch_id = f"batch_{timestamp}"
    
    csv_filename = f"../batches/{batch_id}.csv"
    
    # T·∫°o th∆∞ m·ª•c batches n·∫øu ch∆∞a c√≥
    os.makedirs("../batches", exist_ok=True)
    
    # Kh·ªüi t·∫°o stats
    stats = CrawlStats()
    stats.total_keywords = len(keywords)
    
    batch_start_time = datetime.now()
    
    print(f"üî• BATCH CRAWL - HIGH FREQUENCY TEST")
    print("=" * 60)
    print(f"üÜî Batch ID: {batch_id}")
    print(f"‚è∞ Start Time: {batch_start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"üìù Keywords: {len(keywords)}")
    print(f"üìä Articles per keyword: {articles_per_keyword}")
    print(f"üéØ Expected total articles: {len(keywords) * articles_per_keyword}")
    print(f"üìÅ Output file: {csv_filename}")
    print("=" * 60)
    
    all_articles = []
    
    # T·∫°o v√† m·ªü file CSV
    with open(csv_filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
        fieldnames = [
            'batch_id', 'keyword', 'article_number', 'headline', 'link', 
            'date', 'source', 'domain', 'crawl_timestamp', 'keyword_index'
        ]
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        
        # Crawl t·ª´ng keyword
        for keyword_index, keyword in enumerate(keywords, 1):
            print(f"\nüîç [{keyword_index}/{len(keywords)}] Processing: {keyword}")
            
            # Delay ng·∫Øn gi·ªØa c√°c keyword (1-3 gi√¢y)
            if keyword_index > 1:
                delay = random.uniform(1, 3)
                print(f"‚è≥ Short delay: {delay:.1f}s...")
                time.sleep(delay)
            
            # Crawl keyword v·ªõi s·ªë l∆∞·ª£ng b√†i √≠t
            keyword_articles = simple_crawl_rss(keyword, articles_per_keyword, stats)
            
            # Ghi t·ª´ng b√†i b√°o v√†o CSV
            for i, article in enumerate(keyword_articles, 1):
                row = {
                    'batch_id': batch_id,
                    'keyword': keyword,
                    'article_number': i,
                    'headline': article.headline,
                    'link': article.link,
                    'date': article.date,
                    'source': article.source,
                    'domain': extract_domain(article.link),
                    'crawl_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    'keyword_index': keyword_index
                }
                writer.writerow(row)
            
            all_articles.extend(keyword_articles)
            
            print(f"‚úÖ Found {len(keyword_articles)} articles for '{keyword}'")
            
            # Progress update
            progress = (keyword_index / len(keywords)) * 100
            print(f"üìä Progress: {progress:.1f}% | Total articles: {len(all_articles)}")
    
    batch_end_time = datetime.now()
    batch_duration = batch_end_time - batch_start_time
    
    stats.total_articles = len(all_articles)
    
    # In th·ªëng k√™ batch
    print_batch_stats(stats, csv_filename, batch_id, batch_start_time, batch_end_time, batch_duration)
    
    return csv_filename

def print_batch_stats(stats: CrawlStats, csv_filename: str, batch_id: str, start_time: datetime, end_time: datetime, duration):
    """In th·ªëng k√™ cho batch crawl"""
    print("\n" + "=" * 60)
    print("üéØ BATCH CRAWL RESULTS")
    print("=" * 60)
    print(f"üÜî Batch ID: {batch_id}")
    print(f"‚è∞ Start Time: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"üèÅ End Time: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"‚è±Ô∏è Duration: {str(duration).split('.')[0]}")
    print(f"üìã Keywords Processed: {stats.total_keywords}")
    print(f"üì∞ Total Articles: {stats.total_articles}")
    print(f"‚úÖ Successful Requests: {stats.successful_requests}")
    print(f"‚ùå Failed Requests: {stats.failed_requests}")
    print(f"‚ö†Ô∏è Total Errors: {stats.errors}")
    print(f"üö´ Captcha Skipped: {stats.captcha_skipped}")
    print(f"üìÅ Output File: {csv_filename}")
    
    if stats.total_keywords > 0:
        success_rate = (stats.successful_requests / stats.total_keywords) * 100
        avg_articles = stats.total_articles / stats.total_keywords
        print(f"üìà Success Rate: {success_rate:.1f}%")
        print(f"üìä Avg Articles/Keyword: {avg_articles:.1f}")
        
        # T√≠nh t·ªëc ƒë·ªô crawl
        total_minutes = duration.total_seconds() / 60
        keywords_per_minute = stats.total_keywords / total_minutes if total_minutes > 0 else 0
        articles_per_minute = stats.total_articles / total_minutes if total_minutes > 0 else 0
        
        print(f"‚ö° Crawl Speed: {keywords_per_minute:.1f} keywords/min, {articles_per_minute:.1f} articles/min")
    
    print("=" * 60)
    print("üéâ Batch completed successfully!")

def scheduled_crawler():
    """
    H√†m ch√≠nh ƒë·ªÉ ch·∫°y theo schedule - ƒë∆∞·ª£c g·ªçi b·ªüi task scheduler
    Ch·∫°y m·ªói ti·∫øng 1 l·∫ßn t·ª´ l√∫c k√≠ch ho·∫°t ƒë·∫øn khi d·ª´ng
    """
    
    print(f"üöÄ SCHEDULED CRAWLER STARTED")
    print(f"‚è∞ Current time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    # ƒê·ªçc keywords t·ª´ file CSV
    csv_file_path = "keyword_csvs/Keywords.csv"
    keywords = read_keywords_from_csv(csv_file_path, keyword_column="Keywords")
    
    if not keywords:
        print("‚ùå No keywords loaded from CSV. Exiting...")
        return
    
    # Ch·∫°y batch crawl v·ªõi 10 b√†i m·ªói keyword
    batch_file = batch_crawl_keywords(
        keywords=keywords,
        articles_per_keyword=10
    )
    
    print(f"\nüìÅ Batch file created: {batch_file}")
    print("‚úÖ Scheduled crawl completed!")

def create_batch_schedule_script():
    """
    T·∫°o file .bat ƒë·ªÉ ch·∫°y v·ªõi Windows Task Scheduler
    """
    
    # L·∫•y ƒë∆∞·ªùng d·∫´n hi·ªán t·∫°i
    current_dir = os.getcwd()
    python_script = os.path.join(current_dir, "pypassCapcha.py")
    
    bat_content = f'''@echo off
cd /d "{current_dir}"
python "{python_script}" --scheduled
pause
'''
    
    bat_filename = "run_scheduled_crawler.bat"
    
    with open(bat_filename, 'w', encoding='utf-8') as f:
        f.write(bat_content)
    
    print(f"üìÅ Created batch file: {bat_filename}")
    print("\nüîß TASK SCHEDULER SETUP INSTRUCTIONS:")
    print("=" * 50)
    print("1. Open Windows Task Scheduler")
    print("2. Create Basic Task...")
    print("3. Name: 'Google News Crawler'")
    print("4. Trigger: Daily")
    print("5. Start time: [Choose your preferred start time]")
    print("6. Repeat task every: 1 hour")
    print("7. For a duration of: Indefinitely (or choose your duration)")
    print(f"8. Action: Start a program")
    print(f"9. Program/script: {os.path.abspath(bat_filename)}")
    print("10. Click Finish")
    print("=" * 50)
    print("\nüí° The crawler will run every 1 hour from when you activate it")
    print("üí° Each batch will crawl ~600 articles (60 keywords x 10 articles)")
    print("üí° To stop: Disable or delete the task in Task Scheduler")

def run_manual_test():
    """
    Ch·∫°y test th·ªß c√¥ng ƒë·ªÉ ki·ªÉm tra
    """
    print("üß™ MANUAL TEST MODE")
    print("=" * 40)
    
    # ƒê·ªçc keywords
    csv_file_path = "keyword_csvs/Keywords.csv"
    keywords = read_keywords_from_csv(csv_file_path, keyword_column="Keywords")
    
    if not keywords:
        print("‚ùå No keywords loaded. Exiting...")
        return
    
    # L·∫•y 5 keywords ƒë·∫ßu ti√™n ƒë·ªÉ test nhanh
    test_keywords = keywords[:5]
    print(f"üß™ Testing with {len(test_keywords)} keywords:")
    for i, kw in enumerate(test_keywords, 1):
        print(f"   {i}. {kw}")
    print()
    
    # Ch·∫°y batch test
    batch_file = batch_crawl_keywords(
        keywords=test_keywords,
        articles_per_keyword=10,
        batch_id=f"test_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    )
    
    print(f"\nüìÅ Test batch file: {batch_file}")
    print("‚úÖ Manual test completed!")

def demo_selenium_test():
    """
    Demo function ƒë·ªÉ test Selenium crawler v·ªõi m·ªôt v√†i keywords
    """
    print("ü§ñ SELENIUM DEMO TEST")
    print("=" * 50)
    print("üîß Ki·ªÉm tra Selenium WebDriver...")
    print("üí° L·∫ßn ƒë·∫ßu ch·∫°y s·∫Ω t·ª± ƒë·ªông download ChromeDriver")
    print("‚è≥ Vui l√≤ng ƒë·ª£i...")
    print()
    
    # Test keywords
    test_keywords = ["artificial intelligence", "climate change", "technology"]
    
    print(f"üß™ Testing v·ªõi {len(test_keywords)} keywords:")
    for i, kw in enumerate(test_keywords, 1):
        print(f"   {i}. {kw}")
    print()
    
    all_articles = []
    stats = CrawlStats()
    
    # Test t·ª´ng keyword
    for i, keyword in enumerate(test_keywords, 1):
        print(f"\nüîç [{i}/{len(test_keywords)}] Testing: {keyword}")
        print("-" * 40)
        
        try:
            # Test Selenium
            articles = simple_crawl_with_selenium(keyword, max_results=5, stats=stats)
            
            if articles:
                print(f"‚úÖ Selenium th√†nh c√¥ng: {len(articles)} b√†i")
                for j, article in enumerate(articles[:3], 1):  # Show first 3
                    print(f"   {j}. {article.headline[:60]}...")
                    print(f"      üîó {article.link}")
                all_articles.extend(articles)
            else:
                print(f"‚ùå Selenium th·∫•t b·∫°i cho '{keyword}'")
            
        except Exception as e:
            print(f"‚ùå L·ªói test '{keyword}': {str(e)}")
            stats.errors += 1
        
        # Delay gi·ªØa c√°c test
        if i < len(test_keywords):
            time.sleep(2)
    
    # K·∫øt qu·∫£ t·ªïng
    print("\n" + "=" * 50)
    print("üéØ K·∫æT QU·∫¢ DEMO TEST")
    print("=" * 50)
    print(f"üìä Total articles: {len(all_articles)}")
    print(f"‚úÖ Successful requests: {stats.successful_requests}")
    print(f"‚ùå Failed requests: {stats.failed_requests}")
    print(f"‚ö†Ô∏è Errors: {stats.errors}")
    
    if len(all_articles) > 0:
        print("üéâ Selenium ho·∫°t ƒë·ªông t·ªët!")
        print("üí° C√≥ th·ªÉ s·ª≠ d·ª•ng Selenium mode trong bulk crawl")
        
        # Export demo results
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        demo_file = f"selenium_demo_{timestamp}.csv"
        
        with open(demo_file, 'w', newline='', encoding='utf-8-sig') as csvfile:
            fieldnames = ['keyword', 'headline', 'link', 'date', 'source', 'domain']
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            
            for article in all_articles:
                for kw in test_keywords:
                    if any(word.lower() in article.headline.lower() for word in kw.split()):
                        writer.writerow({
                            'keyword': kw,
                            'headline': article.headline,
                            'link': article.link,
                            'date': article.date,
                            'source': article.source,
                            'domain': extract_domain(article.link)
                        })
                        break
        
        print(f"üìÅ Demo results exported to: {demo_file}")
    else:
        print("‚ùå Selenium kh√¥ng ho·∫°t ƒë·ªông. Ki·ªÉm tra:")
        print("   - Chrome browser ƒë√£ c√†i ƒë·∫∑t?")
        print("   - Internet connection?")
        print("   - Firewall/antivirus blocking?")

def create_selenium_requirements():
    """T·∫°o file requirements.txt cho Selenium"""
    requirements = """# Core packages
requests==2.31.0
beautifulsoup4==4.12.2
fake-useragent==1.4.0
lxml==4.9.3

# Selenium packages
selenium==4.15.2
webdriver-manager==4.0.1

# Additional packages
python-dateutil==2.8.2
urllib3==2.0.7
"""
    
    with open('requirements_selenium.txt', 'w', encoding='utf-8') as f:
        f.write(requirements)
    
    print("üìÅ Created requirements_selenium.txt")
    print("\nüîß SELENIUM SETUP INSTRUCTIONS:")
    print("=" * 40)
    print("1. Install required packages:")
    print("   pip install -r requirements_selenium.txt")
    print("\n2. Make sure Chrome browser is installed")
    print("\n3. First run will auto-download ChromeDriver")
    print("\n4. Run demo test:")
    print("   python pypassCapcha.py --selenium-demo")

def bulk_crawl_with_selenium(keywords: List[str], articles_per_keyword: int = 20, csv_filename: str = None) -> str:
    """
    Bulk crawl s·ª≠ d·ª•ng Selenium cho URLs th·ª±c t·∫ø
    
    Args:
        keywords: Danh s√°ch keywords
        articles_per_keyword: S·ªë b√†i m·ªói keyword
        csv_filename: T√™n file CSV output
    
    Returns:
        T√™n file CSV ƒë√£ t·∫°o
    """
    
    # T·∫°o t√™n file CSV v·ªõi timestamp
    if csv_filename is None:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        csv_filename = f"selenium_crawl_{timestamp}.csv"
    
    # Kh·ªüi t·∫°o stats
    stats = CrawlStats()
    stats.total_keywords = len(keywords)
    
    print(f"ü§ñ SELENIUM BULK CRAWLER")
    print("=" * 60)
    print(f"üìù Keywords: {len(keywords)}")
    print(f"üìä Articles per keyword: {articles_per_keyword}")
    print(f"üéØ Target articles: {len(keywords) * articles_per_keyword}")
    print(f"üìÅ Output file: {csv_filename}")
    print("‚ö†Ô∏è L∆∞u √Ω: Selenium ch·∫≠m h∆°n RSS nh∆∞ng c√≥ URLs th·ª±c t·∫ø")
    print("=" * 60)
    
    all_articles = []
    
    # T·∫°o v√† m·ªü file CSV
    with open(csv_filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
        fieldnames = [
            'keyword', 'article_number', 'headline', 'link', 
            'date', 'source', 'domain', 'crawl_timestamp', 'method'
        ]
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        
        # Crawl t·ª´ng keyword
        for keyword_index, keyword in enumerate(keywords, 1):
            print(f"\nüîç [{keyword_index}/{len(keywords)}] Processing: {keyword}")
            
            # Delay gi·ªØa c√°c keywords (longer cho Selenium)
            if keyword_index > 1:
                delay = random.uniform(10, 20)
                print(f"‚è≥ Selenium delay: {delay:.1f}s...")
                time.sleep(delay)
            
            # Crawl v·ªõi smart fallback (Selenium first)
            keyword_articles = smart_crawl_with_fallback(
                keyword, 
                articles_per_keyword, 
                stats, 
                use_selenium_first=True
            )
            
            # Ghi t·ª´ng b√†i b√°o v√†o CSV
            for i, article in enumerate(keyword_articles, 1):
                row = {
                    'keyword': keyword,
                    'article_number': i,
                    'headline': article.headline,
                    'link': article.link,
                    'date': article.date,
                    'source': article.source,
                    'domain': extract_domain(article.link),
                    'crawl_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    'method': 'Selenium+RSS'
                }
                writer.writerow(row)
            
            all_articles.extend(keyword_articles)
            
            # Progress update
            progress = (keyword_index / len(keywords)) * 100
            print(f"‚úÖ Completed '{keyword}': {len(keyword_articles)} articles")
            print(f"üìä Progress: {progress:.1f}% | Total: {len(all_articles)} articles")
    
    stats.total_articles = len(all_articles)
    
    # In th·ªëng k√™ cu·ªëi c√πng
    print_selenium_bulk_stats(stats, csv_filename, keywords, articles_per_keyword)
    
    return csv_filename

def print_selenium_bulk_stats(stats: CrawlStats, csv_filename: str, keywords: List[str], target_per_keyword: int):
    """In th·ªëng k√™ cho Selenium bulk crawl"""
    print("\n" + "=" * 60)
    print("ü§ñ SELENIUM BULK CRAWL RESULTS")
    print("=" * 60)
    print(f"üìù Keywords processed: {len(keywords)}")
    print(f"üì∞ Total articles: {stats.total_articles}")
    print(f"üéØ Target articles: {len(keywords) * target_per_keyword}")
    print(f"üìä Average per keyword: {stats.total_articles / len(keywords):.1f}")
    print(f"‚úÖ Successful requests: {stats.successful_requests}")
    print(f"‚ùå Failed requests: {stats.failed_requests}")
    print(f"‚ö†Ô∏è Errors: {stats.errors}")
    print(f"üìÅ Output file: {csv_filename}")
    
    # Success rate
    if len(keywords) > 0:
        success_rate = (stats.successful_requests / len(keywords)) * 100
        print(f"üìà Success rate: {success_rate:.1f}%")
    
    print("=" * 60)
    print("üéâ Selenium bulk crawl completed!")
    print("üí° URLs trong file CSV l√† URLs th·ª±c t·∫ø (kh√¥ng ph·∫£i Google redirect)")

def main():
    """H√†m ch√≠nh ƒë∆∞·ª£c c·∫≠p nh·∫≠t ƒë·ªÉ h·ªó tr·ª£ Selenium v√† c√°c mode kh√°c nhau"""
    
    import sys
    
    # Ki·ªÉm tra command line arguments
    if len(sys.argv) > 1:
        if sys.argv[1] == '--scheduled':
            # Mode ch·∫°y theo schedule
            scheduled_crawler()
            return
        elif sys.argv[1] == '--test':
            # Mode test th·ªß c√¥ng
            run_manual_test()
            return
        elif sys.argv[1] == '--setup':
            # Mode setup task scheduler
            create_batch_schedule_script()
            return
        elif sys.argv[1] == '--selenium-demo':
            # Mode demo Selenium
            demo_selenium_test()
            return
        elif sys.argv[1] == '--selenium-setup':
            # Mode setup Selenium requirements
            create_selenium_requirements()
            return
    
    # Mode m·∫∑c ƒë·ªãnh - h·ªèi user
    print("üî• GOOGLE NEWS BATCH CRAWLER")
    print("=" * 50)
    print("Select mode:")
    print("1. Manual test (5 keywords x 10 articles)")
    print("2. Create scheduler setup")
    print("3. Run scheduled crawl")
    print("4. Full crawl (RSS mode)")
    print("5. ü§ñ Selenium demo test")
    print("6. ü§ñ Selenium bulk crawl")
    print("7. üîß Setup Selenium requirements")
    
    choice = input("\nEnter your choice (1-7): ").strip()
    
    if choice == '1':
        run_manual_test()
    elif choice == '2':
        create_batch_schedule_script()
    elif choice == '3':
        scheduled_crawler()
    elif choice == '4':
        # Mode crawl ƒë·∫ßy ƒë·ªß nh∆∞ c≈© (RSS)
        csv_file_path = "keyword_csvs/Keywords.csv"
        keywords = read_keywords_from_csv(csv_file_path, keyword_column="Keywords")
        
        if not keywords:
            print("‚ùå No keywords loaded from CSV. Please check your file.")
            return
        
        target_articles_per_keyword = 200
        csv_file = crawl_multiple_keywords_deep(
            keywords=keywords,
            target_articles_per_keyword=target_articles_per_keyword
        )
        
        print(f"\nüìÅ Data exported to: {csv_file}")
    elif choice == '5':
        # Selenium demo test
        demo_selenium_test()
    elif choice == '6':
        # Selenium bulk crawl
        csv_file_path = "keyword_csvs/Keywords.csv"
        keywords = read_keywords_from_csv(csv_file_path, keyword_column="Keywords")
        
        if not keywords:
            print("‚ùå No keywords loaded from CSV. Please check your file.")
            return
        
        # H·ªèi s·ªë b√†i per keyword
        try:
            articles_per_keyword = int(input("S·ªë b√†i m·ªói keyword (default 20): ") or "20")
        except:
            articles_per_keyword = 20
        
        csv_file = bulk_crawl_with_selenium(
            keywords=keywords,
            articles_per_keyword=articles_per_keyword
        )
        
        print(f"\nüìÅ Selenium data exported to: {csv_file}")
    elif choice == '7':
        # Setup Selenium requirements
        create_selenium_requirements()
    else:
        print("‚ùå Invalid choice. Exiting...")

if __name__ == "__main__":
    main()