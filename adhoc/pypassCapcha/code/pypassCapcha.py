import requests
import time
import random
from bs4 import BeautifulSoup
import json
from urllib.parse import urlencode, quote
from fake_useragent import UserAgent
import itertools
from dataclasses import dataclass
from typing import List, Dict, Optional
import threading
from concurrent.futures import ThreadPoolExecutor
import logging
import csv
from datetime import datetime, time as dt_time
import os

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class NewsArticle:
    headline: str
    link: str
    date: str
    source: str = ""

@dataclass
class CrawlStats:
    total_keywords: int = 0
    total_articles: int = 0
    captcha_skipped: int = 0
    errors: int = 0
    successful_requests: int = 0
    failed_requests: int = 0

class GoogleNewsScraper:
    def __init__(self, proxy_list: List[str] = None):
        """
        Initialize scraper with anti-CAPTCHA measures
        
        Args:
            proxy_list: List of proxy servers in format 'ip:port' or 'username:password@ip:port'
        """
        self.ua = UserAgent()
        self.proxy_list = proxy_list or []
        self.proxy_cycle = itertools.cycle(self.proxy_list) if self.proxy_list else None
        self.session_pool = {}
        self.request_count = 0
        self.last_request_time = 0
        
        # Rate limiting settings
        self.min_delay = 2  # Minimum delay between requests
        self.max_delay = 5  # Maximum delay between requests
        self.requests_per_session = 10  # Switch session after N requests
        
    def get_rotating_session(self) -> requests.Session:
        """Get a session with rotating proxy and headers"""
        session_id = self.request_count // self.requests_per_session
        
        if session_id not in self.session_pool:
            session = requests.Session()
            
            # Set rotating proxy
            if self.proxy_cycle:
                proxy = next(self.proxy_cycle)
                if '@' in proxy:
                    # Authenticated proxy
                    auth, server = proxy.split('@')
                    username, password = auth.split(':')
                    session.auth = (username, password)
                    session.proxies = {
                        'http': f'http://{server}',
                        'https': f'https://{server}'
                    }
                else:
                    # Simple proxy
                    session.proxies = {
                        'http': f'http://{proxy}',
                        'https': f'https://{proxy}'
                    }
            
            # Set realistic headers
            session.headers.update(self._get_realistic_headers())
            
            self.session_pool[session_id] = session
            
        return self.session_pool[session_id]
    
    def _get_realistic_headers(self) -> Dict[str, str]:
        """Generate realistic browser headers"""
        return {
            'User-Agent': self.ua.random,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9,vi;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0',
        }
    
    def _smart_delay(self):
        """Implement smart delay to avoid rate limiting"""
        current_time = time.time()
        elapsed = current_time - self.last_request_time
        
        # Base delay + random jitter
        delay = random.uniform(self.min_delay, self.max_delay)
        
        # Add extra delay if requests are too frequent
        if elapsed < 1:
            delay += random.uniform(2, 4)
        
        # Occasional longer pause to appear more human-like
        if random.random() < 0.1:  # 10% chance
            delay += random.uniform(10, 20)
            
        logger.info(f"Waiting {delay:.2f} seconds...")
        time.sleep(delay)
        self.last_request_time = time.time()
    
    def _crawl_via_crawlbase(self, url: str) -> str:
        """Crawl trang web s·ª≠ d·ª•ng Crawlbase API v·ªõi normal token"""
        try:
            # S·ª≠ d·ª•ng normal token
            normal_token = "xKfElFt5FvM613yeR4Pz8A"
            api_url = f"https://api.crawlbase.com/?token={normal_token}&url={url}"
            
            # Th√™m headers c·∫ßn thi·∫øt
            headers = {
                'Accept-Encoding': 'gzip',
                'User-Agent': self.ua.random,
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.9',
            }
            
            logger.info(f"Calling Crawlbase API for URL: {url}")
            response = requests.get(api_url, headers=headers, timeout=90)
            
            # Log response status v√† headers
            logger.info(f"Crawlbase API Response Status: {response.status_code}")
            logger.info("Crawlbase Response Headers:")
            for key, value in response.headers.items():
                logger.info(f"{key}: {value}")
            
            response.raise_for_status()
            
            # X·ª≠ l√Ω response content
            if 'gzip' in response.headers.get('Content-Encoding', '').lower():
                import gzip
                content = gzip.decompress(response.content)
                html = content.decode('utf-8')
            else:
                html = response.text
            
            # In ra 500 k√Ω t·ª± ƒë·∫ßu c·ªßa HTML ƒë·ªÉ ki·ªÉm tra
            logger.info("First 500 characters of response:")
            logger.info(html[:500])
            
            return html
            
        except Exception as e:
            logger.error(f"Crawlbase API error: {str(e)}")
            return ""

    def search_google_news(self, keyword: str, max_results: int = 20) -> List[NewsArticle]:
        """
        Search Google News for a specific keyword
        
        Args:
            keyword: Search term
            max_results: Maximum number of results to return
            
        Returns:
            List of NewsArticle objects
        """
        articles = []
        
        try:
            # Build search URL
            params = {
                'q': keyword,
                'tbm': 'nws',  # News search
                'num': min(max_results, 100),  # Google limits to 100 per page
                'hl': 'en',
                'gl': 'us'
            }
            
            url = f"https://www.google.com/search?{urlencode(params)}"
            
            # Apply smart delay
            self._smart_delay()
            
            # Get session with rotation
            session = self.get_rotating_session()
            
            logger.info(f"Searching for: {keyword}")
            logger.info(f"Request #{self.request_count + 1}")
            
            # Make request
            response = session.get(url, timeout=30)
            response.raise_for_status()
            
            self.request_count += 1
            
            # Check for CAPTCHA
            if self._is_captcha_page(response.text):
                logger.warning("CAPTCHA detected! Switching to Crawlbase API...")
                html = self._crawl_via_crawlbase(url)
                if html:
                    articles = self._parse_google_news_results(html)
                else:
                    logger.warning("Crawlbase API failed, falling back to RSS...")
                    return self._handle_captcha_fallback(keyword, max_results)
            else:
                # Parse results
                articles = self._parse_google_news_results(response.text)
            
            logger.info(f"Found {len(articles)} articles for '{keyword}'")
            
        except Exception as e:
            logger.error(f"Error searching for '{keyword}': {str(e)}")
            
        return articles[:max_results]
    
    def _is_captcha_page(self, html: str) -> bool:
        """Check if the response contains a CAPTCHA"""
        captcha_indicators = [
            'captcha',
            'unusual traffic',
            'automated queries',
            'recaptcha',
            'verify you are human'
        ]
        
        html_lower = html.lower()
        return any(indicator in html_lower for indicator in captcha_indicators)
    
    def _handle_captcha_fallback(self, keyword: str, max_results: int) -> List[NewsArticle]:
        """Fallback strategy when CAPTCHA is encountered"""
        logger.info("Implementing CAPTCHA fallback strategy...")
        
        # Strategy 1: Switch to a different proxy/session
        if self.proxy_list:
            logger.info("Switching to next proxy...")
            # Force new session
            session_id = self.request_count // self.requests_per_session
            if session_id in self.session_pool:
                del self.session_pool[session_id]
        
        # Strategy 2: Extended delay
        delay = random.uniform(60, 120)  # 1-2 minutes
        logger.info(f"Extended delay: {delay:.2f} seconds")
        time.sleep(delay)
        
        # Strategy 3: Try alternative approach (RSS feeds)
        return self._search_via_rss(keyword, max_results)
    
    def _search_via_rss(self, keyword: str, max_results: int) -> List[NewsArticle]:
        """Alternative search using Google News RSS"""
        try:
            # Google News RSS URL
            rss_url = f"https://news.google.com/rss/search?q={urlencode({'': keyword})[1:]}&hl=en&gl=us"
            
            session = self.get_rotating_session()
            response = session.get(rss_url, timeout=30)
            response.raise_for_status()
            
            # In ra 500 k√Ω t·ª± ƒë·∫ßu c·ªßa RSS response ƒë·ªÉ debug
            logger.warning("In ra 500 k√Ω t·ª± ƒë·∫ßu c·ªßa RSS response ƒë·ªÉ ki·ªÉm tra...")
            print("\n===== RSS RESPONSE (500 k√Ω t·ª± ƒë·∫ßu) =====\n")
            print(response.text[:500])
            print("\n========================================\n")
            
            # Parse RSS feed
            soup = BeautifulSoup(response.content, 'xml')
            articles = []
            
            for item in soup.find_all('item')[:max_results]:
                try:
                    article = NewsArticle(
                        headline=item.title.text if item.title else "N/A",
                        link=item.link.text if item.link else "N/A",
                        date=item.pubDate.text if item.pubDate else "N/A",
                        source=item.source.text if item.source else "N/A"
                    )
                    articles.append(article)
                except Exception as e:
                    logger.warning(f"Error parsing RSS item: {e}")
                    continue
                    
            logger.info(f"RSS fallback found {len(articles)} articles")
            return articles
            
        except Exception as e:
            logger.error(f"RSS fallback failed: {e}")
            return []
    
    def _parse_google_news_results(self, html: str) -> List[NewsArticle]:
        """Parse Google News search results"""
        soup = BeautifulSoup(html, 'html.parser')
        articles = []
        
        # Google News result selectors (these may need updates)
        selectors = [
            'div[data-ved] a[href*="/url?"]',  # Standard news results
            'div.g a h3',  # Alternative selector
            'div.SoaBEf a',  # Another common selector
        ]
        
        for selector in selectors:
            elements = soup.select(selector)
            if elements:
                break
        else:
            # N·∫øu kh√¥ng t√¨m th·∫•y elements n√†o, in ra m·ªôt ph·∫ßn n·ªôi dung HTML ƒë·ªÉ debug
            logger.warning("Kh√¥ng t√¨m th·∫•y elements ph√π h·ª£p! In ra 500 k√Ω t·ª± ƒë·∫ßu c·ªßa HTML ƒë·ªÉ ki·ªÉm tra...")
            print("\n===== HTML RESPONSE (500 k√Ω t·ª± ƒë·∫ßu) =====\n")
            print(html[:500])
            print("\n========================================\n")
            return []
        
        for element in elements:
            try:
                # Extract headline
                headline_elem = element.find('h3') or element
                headline = headline_elem.get_text(strip=True) if headline_elem else "N/A"
                
                # Extract link
                link = element.get('href', '')
                if link.startswith('/url?q='):
                    # Decode Google redirect URL
                    link = link.split('/url?q=')[1].split('&')[0]
                
                # Extract date (this is tricky with Google's structure)
                date_elem = element.find_parent().find(text=lambda x: x and any(
                    term in x.lower() for term in ['ago', 'hours', 'days', 'minutes']
                ))
                date = date_elem.strip() if date_elem else "N/A"
                
                if headline != "N/A" and link:
                    article = NewsArticle(
                        headline=headline,
                        link=link,
                        date=date
                    )
                    articles.append(article)
                    
            except Exception as e:
                logger.warning(f"Error parsing article: {e}")
                continue
                
        return articles
    
    def scrape_multiple_keywords(self, keywords: List[str], max_results_per_keyword: int = 20) -> Dict[str, List[NewsArticle]]:
        """
        Scrape Google News for multiple keywords
        
        Args:
            keywords: List of search terms
            max_results_per_keyword: Maximum results per keyword
            
        Returns:
            Dictionary mapping keywords to their articles
        """
        results = {}
        
        logger.info(f"Starting scraping for {len(keywords)} keywords...")
        
        for i, keyword in enumerate(keywords, 1):
            logger.info(f"Processing keyword {i}/{len(keywords)}: {keyword}")
            
            articles = self.search_google_news(keyword, max_results_per_keyword)
            results[keyword] = articles
            
            # Progress update
            if i % 10 == 0:
                logger.info(f"Completed {i}/{len(keywords)} keywords")
        
        logger.info("Scraping completed!")
        return results
    
    def export_results(self, results: Dict[str, List[NewsArticle]], filename: str = "google_news_results.json"):
        """Export results to JSON file"""
        export_data = {}
        
        for keyword, articles in results.items():
            export_data[keyword] = [
                {
                    'headline': article.headline,
                    'link': article.link,
                    'date': article.date,
                    'source': article.source
                }
                for article in articles
            ]
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"Results exported to {filename}")

def generate_keyword_variations(base_keyword: str, max_pages: int = 10) -> List[str]:
    """
    Tr·∫£ v·ªÅ keyword g·ªëc - kh√¥ng t·∫°o variations, ch·ªâ l·∫•y b√†i m·ªõi nh·∫•t
    
    Args:
        base_keyword: Keyword g·ªëc
        max_pages: Kh√¥ng s·ª≠ d·ª•ng, ch·ªâ gi·ªØ ƒë·ªÉ compatibility
    
    Returns:
        List ch·ªâ ch·ª©a keyword g·ªëc
    """
    # Ch·ªâ tr·∫£ v·ªÅ keyword g·ªëc, kh√¥ng t·∫°o variations
    return [base_keyword]

def crawl_keyword_multiple_pages(keyword: str, pages_per_keyword: int = 10, articles_per_page: int = 20, stats: CrawlStats = None) -> List[NewsArticle]:
    """
    Crawl m·ªôt keyword - ch·ªâ l·∫•y b√†i m·ªõi nh·∫•t kh√¥ng d√πng variations
    
    Args:
        keyword: Keyword g·ªëc
        pages_per_keyword: Kh√¥ng s·ª≠ d·ª•ng, ch·ªâ gi·ªØ ƒë·ªÉ compatibility
        articles_per_page: S·ªë b√†i mu·ªën l·∫•y
        stats: Object th·ªëng k√™
    
    Returns:
        List b√†i b√°o m·ªõi nh·∫•t cho keyword
    """
    if stats is None:
        stats = CrawlStats()
    
    print(f"üîç Crawling latest articles for keyword: {keyword}")
    
    # Ch·ªâ crawl keyword g·ªëc, l·∫•y nhi·ªÅu b√†i h∆°n ƒë·ªÉ ƒë·∫£m b·∫£o c√≥ ƒë·ªß b√†i m·ªõi nh·∫•t
    articles = simple_crawl_rss(keyword, articles_per_page, stats)
    
    print(f"üéØ Found {len(articles)} articles for '{keyword}'")
    return articles

def simple_crawl_rss(keyword: str, max_results: int = 20, stats: CrawlStats = None) -> List[NewsArticle]:
    """Crawl Google News RSS ƒë∆°n gi·∫£n - √≠t b·ªã ch·∫∑n"""
    if stats is None:
        stats = CrawlStats()
    
    try:
        # Google News RSS URL v·ªõi keyword
        encoded_keyword = quote(keyword)
        rss_url = f"https://news.google.com/rss/search?q={encoded_keyword}&hl=en&gl=us&ceid=US:en"
        
        # Headers ƒë∆°n gi·∫£n
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        # G·ªçi RSS
        response = requests.get(rss_url, headers=headers, timeout=30)
        response.raise_for_status()
        
        # Parse RSS v·ªõi lxml ho·∫∑c html.parser l√†m fallback
        try:
            soup = BeautifulSoup(response.content, 'xml')
        except:
            try:
                soup = BeautifulSoup(response.content, 'lxml-xml')
            except:
                soup = BeautifulSoup(response.content, 'html.parser')
        
        articles = []
        
        # T√¨m t·∫•t c·∫£ c√°c item trong RSS
        items = soup.find_all('item')
        
        for i, item in enumerate(items[:max_results]):
            try:
                title = item.title.text if item.title else "N/A"
                link = item.link.text if item.link else "N/A"
                pub_date = item.pubDate.text if item.pubDate else "N/A"
                
                # L·∫•y source t·ª´ description ho·∫∑c source tag
                source = "N/A"
                if item.source:
                    source = item.source.text
                elif item.description:
                    desc = item.description.text
                    # Extract source t·ª´ description n·∫øu c√≥
                    import re
                    source_match = re.search(r'<a[^>]*>([^<]+)</a>', desc)
                    if source_match:
                        source = source_match.group(1)
                
                article = NewsArticle(
                    headline=title,
                    link=link,
                    date=pub_date,
                    source=source
                )
                articles.append(article)
                
            except Exception as e:
                stats.errors += 1
                continue
        
        stats.successful_requests += 1
        return articles
        
    except Exception as e:
        stats.failed_requests += 1
        stats.errors += 1
        return []

def extract_domain(url: str) -> str:
    """Tr√≠ch xu·∫•t domain t·ª´ URL"""
    try:
        if not url or url == 'N/A':
            return 'N/A'
        
        from urllib.parse import urlparse
        parsed = urlparse(url)
        domain = parsed.netloc
        
        # Lo·∫°i b·ªè 'www.' n·∫øu c√≥
        if domain.startswith('www.'):
            domain = domain[4:]
            
        return domain
    except:
        return 'N/A'

def bulk_crawl_to_csv(keywords: List[str], pages_per_keyword: int = 10, articles_per_page: int = 20, csv_filename: str = None) -> str:
    """
    Crawl nhi·ªÅu keyword v·ªõi nhi·ªÅu trang m·ªói keyword v√† xu·∫•t ra CSV
    
    Args:
        keywords: Danh s√°ch t·ª´ kh√≥a
        pages_per_keyword: S·ªë trang m·ªói keyword
        articles_per_page: S·ªë b√†i m·ªói trang
        csv_filename: T√™n file CSV output
    
    Returns:
        T√™n file CSV ƒë√£ t·∫°o
    """
    
    # T·∫°o t√™n file CSV v·ªõi timestamp
    if csv_filename is None:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        csv_filename = f"crawl_results_{timestamp}.csv"
    
    # Kh·ªüi t·∫°o stats
    stats = CrawlStats()
    stats.total_keywords = len(keywords)
    
    estimated_articles = len(keywords) * pages_per_keyword * articles_per_page
    
    print(f"üî• GOOGLE NEWS BULK CRAWLER - MULTI-PAGE MODE")
    print("=" * 70)
    print(f"üìù Keywords to crawl: {len(keywords)}")
    print(f"üìÑ Pages per keyword: {pages_per_keyword}")
    print(f"üìä Articles per page: {articles_per_page}")
    print(f"üéØ Estimated total articles: {estimated_articles}")
    print(f"üìÅ Output file: {csv_filename}")
    print("=" * 70)
    
    # T·∫°o v√† m·ªü file CSV
    with open(csv_filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
        fieldnames = [
            'keyword', 'article_number', 'headline', 'link', 
            'date', 'source', 'domain', 'crawl_timestamp'
        ]
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        
        # Crawl t·ª´ng keyword
        for i, keyword in enumerate(keywords, 1):
            print(f"\nüìã Processing keyword {i}/{len(keywords)}: {keyword}")
            
            # Delay ng·∫´u nhi√™n gi·ªØa c√°c keyword
            if i > 1:
                delay = random.uniform(5, 10)
                print(f"‚è≥ Waiting {delay:.1f} seconds between keywords...")
                time.sleep(delay)
            
            # Crawl nhi·ªÅu trang cho keyword n√†y
            all_articles = crawl_keyword_multiple_pages(
                keyword, 
                pages_per_keyword, 
                articles_per_page, 
                stats
            )
            
            # Ghi t·ª´ng b√†i b√°o v√†o CSV
            for j, article in enumerate(all_articles, 1):
                row = {
                    'keyword': keyword,
                    'article_number': j,
                    'headline': article.headline,
                    'link': article.link,
                    'date': article.date,
                    'source': article.source,
                    'domain': extract_domain(article.link),
                    'crawl_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }
                writer.writerow(row)
            
            stats.total_articles += len(all_articles)
            
            # Progress update
            print(f"üìä Completed {i}/{len(keywords)} keywords. Total articles so far: {stats.total_articles}")
    
    # In th·ªëng k√™ cu·ªëi c√πng
    print_final_stats(stats, csv_filename)
    
    return csv_filename

def print_final_stats(stats: CrawlStats, csv_filename: str):
    """In th·ªëng k√™ cu·ªëi c√πng"""
    print("\n" + "=" * 60)
    print("üéØ CRAWL STATISTICS")
    print("=" * 60)
    print(f"üìã Total Keywords: {stats.total_keywords}")
    print(f"üì∞ Total Articles: {stats.total_articles}")
    print(f"‚úÖ Successful Requests: {stats.successful_requests}")
    print(f"‚ùå Failed Requests: {stats.failed_requests}")
    print(f"‚ö†Ô∏è Total Errors: {stats.errors}")
    print(f"üö´ Captcha Skipped: {stats.captcha_skipped}")
    print(f"üìÅ Output File: {csv_filename}")
    
    if stats.total_keywords > 0:
        success_rate = (stats.successful_requests / stats.total_keywords) * 100
        avg_articles = stats.total_articles / stats.total_keywords if stats.total_keywords > 0 else 0
        print(f"üìà Success Rate: {success_rate:.1f}%")
        print(f"üìä Average Articles per Keyword: {avg_articles:.1f}")
    
    print("=" * 60)
    print("üéâ Crawl completed successfully!")

def crawl_single_keyword_deep(keyword: str, target_articles: int = 200, csv_filename: str = None) -> str:
    """
    Crawl 1 keyword ƒë·ªÉ l·∫•y b√†i m·ªõi nh·∫•t (kh√¥ng d√πng variations)
    
    Args:
        keyword: Keyword mu·ªën crawl
        target_articles: S·ªë b√†i m·ª•c ti√™u
        csv_filename: T√™n file CSV output
    
    Returns:
        T√™n file CSV ƒë√£ t·∫°o
    """
    
    # T·∫°o t√™n file CSV v·ªõi keyword v√† timestamp
    if csv_filename is None:
        safe_keyword = keyword.replace(' ', '_').replace('/', '_').replace('\\', '_')
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        csv_filename = f"{safe_keyword}_{timestamp}.csv"
    
    # Kh·ªüi t·∫°o stats
    stats = CrawlStats()
    stats.total_keywords = 1
    
    print(f"üî• CRAWL LATEST ARTICLES FOR SINGLE KEYWORD")
    print("=" * 60)
    print(f"üéØ Keyword: {keyword}")
    print(f"üìä Target articles: {target_articles}")
    print(f"üìÅ Output file: {csv_filename}")
    print("=" * 60)
    
    print(f"üîç Crawling latest articles for '{keyword}'...")
    
    # Ch·ªâ crawl keyword g·ªëc ƒë·ªÉ l·∫•y b√†i m·ªõi nh·∫•t
    articles = simple_crawl_rss(keyword, target_articles, stats)
    
    # T·∫°o v√† m·ªü file CSV
    with open(csv_filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
        fieldnames = [
            'article_number', 'headline', 'link', 
            'date', 'source', 'domain', 'crawl_timestamp'
        ]
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        
        # Ghi t·ª´ng b√†i b√°o v√†o CSV
        for i, article in enumerate(articles, 1):
            row = {
                'article_number': i,
                'headline': article.headline,
                'link': article.link,
                'date': article.date,
                'source': article.source,
                'domain': extract_domain(article.link),
                'crawl_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
            writer.writerow(row)
    
    stats.total_articles = len(articles)
    
    # In th·ªëng k√™ cu·ªëi c√πng
    print_single_keyword_stats(stats, csv_filename, keyword, len(articles), target_articles)
    
    return csv_filename

def print_single_keyword_stats(stats: CrawlStats, csv_filename: str, keyword: str, actual_articles: int, target_articles: int):
    """In th·ªëng k√™ cho single keyword crawl"""
    print("\n" + "=" * 60)
    print("üéØ CRAWL RESULTS")
    print("=" * 60)
    print(f"üîç Keyword: {keyword}")
    print(f"üì∞ Articles Found: {actual_articles}")
    print(f"üéØ Target Articles: {target_articles}")
    print(f"‚úÖ Successful Requests: {stats.successful_requests}")
    print(f"‚ùå Failed Requests: {stats.failed_requests}")
    print(f"‚ö†Ô∏è Total Errors: {stats.errors}")
    print(f"üö´ Captcha Skipped: {stats.captcha_skipped}")
    print(f"üìÅ Output File: {csv_filename}")
    
    # Progress percentage
    progress = min(actual_articles / target_articles * 100, 100) if target_articles > 0 else 0
    print(f"üìà Target Achievement: {progress:.1f}%")
    
    if actual_articles >= target_articles:
        print("üéâ Target achieved successfully!")
    else:
        print(f"‚ÑπÔ∏è Found {actual_articles} articles (target was {target_articles})")
    
    print("=" * 60)

def crawl_multiple_keywords_deep(keywords: List[str], target_articles_per_keyword: int = 200, csv_filename: str = None) -> str:
    """
    Crawl nhi·ªÅu keywords, m·ªói keyword l·∫•y ~200 b√†i
    
    Args:
        keywords: Danh s√°ch keywords
        target_articles_per_keyword: S·ªë b√†i m·ª•c ti√™u cho m·ªói keyword
        csv_filename: T√™n file CSV output
    
    Returns:
        T√™n file CSV ƒë√£ t·∫°o
    """
    
    # T·∫°o t√™n file CSV v·ªõi timestamp
    if csv_filename is None:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        csv_filename = f"multi_keywords_crawl_{timestamp}.csv"
    
    # Kh·ªüi t·∫°o stats t·ªïng
    total_stats = CrawlStats()
    total_stats.total_keywords = len(keywords)
    
    print(f"üî• MULTI-KEYWORDS DEEP CRAWLER")
    print("=" * 70)
    print(f"üìù Keywords: {len(keywords)}")
    print(f"üìä Target articles per keyword: {target_articles_per_keyword}")
    print(f"üéØ Total target articles: {len(keywords) * target_articles_per_keyword}")
    print(f"üìÅ Output file: {csv_filename}")
    print("=" * 70)
    
    # Hi·ªÉn th·ªã keywords s·∫Ω crawl
    print("üìã KEYWORDS TO CRAWL:")
    for i, keyword in enumerate(keywords, 1):
        print(f"   {i}. {keyword}")
    print()
    
    all_articles = []
    
    # T·∫°o v√† m·ªü file CSV
    with open(csv_filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
        fieldnames = [
            'keyword', 'article_number', 'headline', 'link', 
            'date', 'source', 'domain', 'crawl_timestamp'
        ]
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        
        # Crawl t·ª´ng keyword
        for keyword_index, keyword in enumerate(keywords, 1):
            print(f"\nüîç PROCESSING KEYWORD {keyword_index}/{len(keywords)}: {keyword}")
            print("-" * 50)
            
            # Delay gi·ªØa c√°c keywords
            if keyword_index > 1:
                delay = random.uniform(8, 15)
                print(f"‚è≥ Waiting {delay:.1f} seconds between keywords...")
                time.sleep(delay)
            
            # Crawl keyword n√†y
            keyword_articles = crawl_single_keyword_for_multi(
                keyword, 
                target_articles_per_keyword, 
                total_stats
            )
            
            # Ghi t·∫•t c·∫£ b√†i b√°o c·ªßa keyword n√†y v√†o CSV
            for i, article in enumerate(keyword_articles, 1):
                row = {
                    'keyword': keyword,
                    'article_number': i,
                    'headline': article.headline,
                    'link': article.link,
                    'date': article.date,
                    'source': article.source,
                    'domain': extract_domain(article.link),
                    'crawl_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }
                writer.writerow(row)
            
            all_articles.extend(keyword_articles)
            
            # Progress update
            print(f"‚úÖ Completed {keyword}: {len(keyword_articles)} articles")
            print(f"üìä Overall progress: {keyword_index}/{len(keywords)} keywords, {len(all_articles)} total articles")
    
    total_stats.total_articles = len(all_articles)
    
    # In th·ªëng k√™ cu·ªëi c√πng
    print_multi_keywords_stats(total_stats, csv_filename, keywords, target_articles_per_keyword, len(all_articles))
    
    return csv_filename

def crawl_single_keyword_for_multi(keyword: str, target_articles: int, stats: CrawlStats) -> List[NewsArticle]:
    """
    Crawl 1 keyword cho multi-keyword crawl - ch·ªâ l·∫•y b√†i m·ªõi nh·∫•t
    
    Args:
        keyword: Keyword mu·ªën crawl
        target_articles: S·ªë b√†i m·ª•c ti√™u
        stats: Object th·ªëng k√™ chung
    
    Returns:
        List c√°c b√†i b√°o m·ªõi nh·∫•t
    """
    
    print(f"üîç Crawling latest articles for '{keyword}'...")
    
    # Ch·ªâ crawl keyword g·ªëc v·ªõi s·ªë b√†i ƒë∆∞·ª£c y√™u c·∫ßu
    articles = simple_crawl_rss(keyword, target_articles, stats)
    
    print(f"üéØ Found {len(articles)} articles for '{keyword}'")
    return articles

def print_multi_keywords_stats(stats: CrawlStats, csv_filename: str, keywords: List[str], target_per_keyword: int, total_articles: int):
    """In th·ªëng k√™ cho multi-keywords crawl"""
    print("\n" + "=" * 70)
    print("üéØ MULTI-KEYWORDS CRAWL RESULTS")
    print("=" * 70)
    print(f"üìù Total Keywords: {len(keywords)}")
    print(f"üì∞ Total Articles Found: {total_articles}")
    print(f"üéØ Target Articles: {len(keywords) * target_per_keyword}")
    print(f"üìä Average Articles per Keyword: {total_articles / len(keywords):.1f}")
    print(f"‚úÖ Successful Requests: {stats.successful_requests}")
    print(f"‚ùå Failed Requests: {stats.failed_requests}")
    print(f"‚ö†Ô∏è Total Errors: {stats.errors}")
    print(f"üö´ Captcha Skipped: {stats.captcha_skipped}")
    print(f"üìÅ Output File: {csv_filename}")
    
    # Progress percentage
    target_total = len(keywords) * target_per_keyword
    progress = min(total_articles / target_total * 100, 100) if target_total > 0 else 0
    print(f"üìà Overall Achievement: {progress:.1f}%")
    
    if total_articles >= target_total:
        print("üéâ All targets achieved successfully!")
    else:
        print(f"‚ÑπÔ∏è Found {total_articles} articles (target was {target_total})")
    
    print("=" * 70)
    print("üéâ Multi-keywords crawl completed!")

def read_keywords_from_csv(csv_file_path: str, keyword_column: str = None, skip_header: bool = True) -> List[str]:
    """
    ƒê·ªçc keywords t·ª´ file CSV
    
    Args:
        csv_file_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file CSV ch·ª©a keywords
        keyword_column: T√™n c·ªôt ch·ª©a keywords (n·∫øu None s·∫Ω l·∫•y c·ªôt ƒë·∫ßu ti√™n)
        skip_header: C√≥ b·ªè qua header row kh√¥ng
    
    Returns:
        List c√°c keywords t·ª´ CSV
    """
    keywords = []
    
    try:
        print(f"üìñ Reading keywords from CSV file: {csv_file_path}")
        
        with open(csv_file_path, 'r', encoding='utf-8-sig') as csvfile:
            # Detect delimiter
            sample = csvfile.read(1024)
            csvfile.seek(0)
            
            delimiter = ','
            if ';' in sample and sample.count(';') > sample.count(','):
                delimiter = ';'
            elif '\t' in sample:
                delimiter = '\t'
            
            print(f"üìä Detected CSV delimiter: '{delimiter}'")
            
            reader = csv.reader(csvfile, delimiter=delimiter)
            
            # Skip header if needed
            if skip_header:
                try:
                    header = next(reader)
                    print(f"üìã CSV Header: {header}")
                    
                    # T·ª± ƒë·ªông detect keyword column n·∫øu kh√¥ng ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh
                    if keyword_column is None:
                        keyword_column = header[0]  # L·∫•y c·ªôt ƒë·∫ßu ti√™n
                        keyword_col_index = 0
                    else:
                        # T√¨m index c·ªßa keyword column
                        keyword_col_index = None
                        for i, col in enumerate(header):
                            if col.lower().strip() == keyword_column.lower().strip():
                                keyword_col_index = i
                                break
                        
                        if keyword_col_index is None:
                            print(f"‚ö†Ô∏è Column '{keyword_column}' not found. Using first column.")
                            keyword_col_index = 0
                    
                    print(f"üéØ Using column '{header[keyword_col_index]}' (index: {keyword_col_index}) for keywords")
                    
                except StopIteration:
                    print("‚ö†Ô∏è CSV file is empty")
                    return []
            else:
                keyword_col_index = 0
            
            # ƒê·ªçc keywords
            for row_num, row in enumerate(reader, start=2 if skip_header else 1):
                if row and len(row) > keyword_col_index:
                    keyword = row[keyword_col_index].strip()
                    if keyword:  # B·ªè qua d√≤ng tr·ªëng
                        keywords.append(keyword)
                        print(f"   ‚úÖ Row {row_num}: '{keyword}'")
                else:
                    print(f"   ‚ö†Ô∏è Row {row_num}: Empty or invalid row")
        
        print(f"üìä Total keywords loaded: {len(keywords)}")
        return keywords
        
    except FileNotFoundError:
        print(f"‚ùå File not found: {csv_file_path}")
        return []
    except Exception as e:
        print(f"‚ùå Error reading CSV file: {str(e)}")
        return []

def is_within_working_hours() -> bool:
    """
    Ki·ªÉm tra xem hi·ªán t·∫°i c√≥ trong gi·ªù l√†m vi·ªác kh√¥ng (10h - 18h)
    
    Returns:
        True n·∫øu trong gi·ªù l√†m vi·ªác, False n·∫øu kh√¥ng
    """
    now = datetime.now()
    current_time = now.time()
    
    # Gi·ªù l√†m vi·ªác: 10:00 - 18:00
    start_time = dt_time(10, 0)  # 10:00 AM
    end_time = dt_time(18, 0)    # 6:00 PM
    
    return start_time <= current_time <= end_time

def batch_crawl_keywords(keywords: List[str], articles_per_keyword: int = 10, batch_id: str = None) -> str:
    """
    Crawl keywords theo batch v·ªõi s·ªë l∆∞·ª£ng b√†i √≠t ƒë·ªÉ test t·∫ßn su·∫•t cao
    
    Args:
        keywords: Danh s√°ch keywords
        articles_per_keyword: S·ªë b√†i m·ªói keyword (m·∫∑c ƒë·ªãnh 10)
        batch_id: ID c·ªßa batch (n·∫øu None s·∫Ω t·ª± t·∫°o t·ª´ timestamp)
    
    Returns:
        T√™n file CSV ƒë√£ t·∫°o
    """
    
    # T·∫°o batch ID v√† t√™n file
    if batch_id is None:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        batch_id = f"batch_{timestamp}"
    
    csv_filename = f"../batches/{batch_id}.csv"
    
    # T·∫°o th∆∞ m·ª•c batches n·∫øu ch∆∞a c√≥
    os.makedirs("../batches", exist_ok=True)
    
    # Kh·ªüi t·∫°o stats
    stats = CrawlStats()
    stats.total_keywords = len(keywords)
    
    batch_start_time = datetime.now()
    
    print(f"üî• BATCH CRAWL - HIGH FREQUENCY TEST")
    print("=" * 60)
    print(f"üÜî Batch ID: {batch_id}")
    print(f"‚è∞ Start Time: {batch_start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"üìù Keywords: {len(keywords)}")
    print(f"üìä Articles per keyword: {articles_per_keyword}")
    print(f"üéØ Expected total articles: {len(keywords) * articles_per_keyword}")
    print(f"üìÅ Output file: {csv_filename}")
    print("=" * 60)
    
    all_articles = []
    
    # T·∫°o v√† m·ªü file CSV
    with open(csv_filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
        fieldnames = [
            'batch_id', 'keyword', 'article_number', 'headline', 'link', 
            'date', 'source', 'domain', 'crawl_timestamp', 'keyword_index'
        ]
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        
        # Crawl t·ª´ng keyword
        for keyword_index, keyword in enumerate(keywords, 1):
            print(f"\nüîç [{keyword_index}/{len(keywords)}] Processing: {keyword}")
            
            # Delay ng·∫Øn gi·ªØa c√°c keyword (1-3 gi√¢y)
            if keyword_index > 1:
                delay = random.uniform(1, 3)
                print(f"‚è≥ Short delay: {delay:.1f}s...")
                time.sleep(delay)
            
            # Crawl keyword v·ªõi s·ªë l∆∞·ª£ng b√†i √≠t
            keyword_articles = simple_crawl_rss(keyword, articles_per_keyword, stats)
            
            # Ghi t·ª´ng b√†i b√°o v√†o CSV
            for i, article in enumerate(keyword_articles, 1):
                row = {
                    'batch_id': batch_id,
                    'keyword': keyword,
                    'article_number': i,
                    'headline': article.headline,
                    'link': article.link,
                    'date': article.date,
                    'source': article.source,
                    'domain': extract_domain(article.link),
                    'crawl_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    'keyword_index': keyword_index
                }
                writer.writerow(row)
            
            all_articles.extend(keyword_articles)
            
            print(f"‚úÖ Found {len(keyword_articles)} articles for '{keyword}'")
            
            # Progress update
            progress = (keyword_index / len(keywords)) * 100
            print(f"üìä Progress: {progress:.1f}% | Total articles: {len(all_articles)}")
    
    batch_end_time = datetime.now()
    batch_duration = batch_end_time - batch_start_time
    
    stats.total_articles = len(all_articles)
    
    # In th·ªëng k√™ batch
    print_batch_stats(stats, csv_filename, batch_id, batch_start_time, batch_end_time, batch_duration)
    
    return csv_filename

def print_batch_stats(stats: CrawlStats, csv_filename: str, batch_id: str, start_time: datetime, end_time: datetime, duration):
    """In th·ªëng k√™ cho batch crawl"""
    print("\n" + "=" * 60)
    print("üéØ BATCH CRAWL RESULTS")
    print("=" * 60)
    print(f"üÜî Batch ID: {batch_id}")
    print(f"‚è∞ Start Time: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"üèÅ End Time: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"‚è±Ô∏è Duration: {str(duration).split('.')[0]}")
    print(f"üìã Keywords Processed: {stats.total_keywords}")
    print(f"üì∞ Total Articles: {stats.total_articles}")
    print(f"‚úÖ Successful Requests: {stats.successful_requests}")
    print(f"‚ùå Failed Requests: {stats.failed_requests}")
    print(f"‚ö†Ô∏è Total Errors: {stats.errors}")
    print(f"üö´ Captcha Skipped: {stats.captcha_skipped}")
    print(f"üìÅ Output File: {csv_filename}")
    
    if stats.total_keywords > 0:
        success_rate = (stats.successful_requests / stats.total_keywords) * 100
        avg_articles = stats.total_articles / stats.total_keywords
        print(f"üìà Success Rate: {success_rate:.1f}%")
        print(f"üìä Avg Articles/Keyword: {avg_articles:.1f}")
        
        # T√≠nh t·ªëc ƒë·ªô crawl
        total_minutes = duration.total_seconds() / 60
        keywords_per_minute = stats.total_keywords / total_minutes if total_minutes > 0 else 0
        articles_per_minute = stats.total_articles / total_minutes if total_minutes > 0 else 0
        
        print(f"‚ö° Crawl Speed: {keywords_per_minute:.1f} keywords/min, {articles_per_minute:.1f} articles/min")
    
    print("=" * 60)
    print("üéâ Batch completed successfully!")

def scheduled_crawler():
    """
    H√†m ch√≠nh ƒë·ªÉ ch·∫°y theo schedule - ƒë∆∞·ª£c g·ªçi b·ªüi task scheduler
    Ch·∫°y m·ªói ti·∫øng 1 l·∫ßn t·ª´ l√∫c k√≠ch ho·∫°t ƒë·∫øn khi d·ª´ng
    """
    
    print(f"üöÄ SCHEDULED CRAWLER STARTED")
    print(f"‚è∞ Current time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    # ƒê·ªçc keywords t·ª´ file CSV
    csv_file_path = "keyword_csvs/Keywords.csv"
    keywords = read_keywords_from_csv(csv_file_path, keyword_column="Keywords")
    
    if not keywords:
        print("‚ùå No keywords loaded from CSV. Exiting...")
        return
    
    # Ch·∫°y batch crawl v·ªõi 10 b√†i m·ªói keyword
    batch_file = batch_crawl_keywords(
        keywords=keywords,
        articles_per_keyword=10
    )
    
    print(f"\nüìÅ Batch file created: {batch_file}")
    print("‚úÖ Scheduled crawl completed!")

def create_batch_schedule_script():
    """
    T·∫°o file .bat ƒë·ªÉ ch·∫°y v·ªõi Windows Task Scheduler
    """
    
    # L·∫•y ƒë∆∞·ªùng d·∫´n hi·ªán t·∫°i
    current_dir = os.getcwd()
    python_script = os.path.join(current_dir, "pypassCapcha.py")
    
    bat_content = f'''@echo off
cd /d "{current_dir}"
python "{python_script}" --scheduled
pause
'''
    
    bat_filename = "run_scheduled_crawler.bat"
    
    with open(bat_filename, 'w', encoding='utf-8') as f:
        f.write(bat_content)
    
    print(f"üìÅ Created batch file: {bat_filename}")
    print("\nüîß TASK SCHEDULER SETUP INSTRUCTIONS:")
    print("=" * 50)
    print("1. Open Windows Task Scheduler")
    print("2. Create Basic Task...")
    print("3. Name: 'Google News Crawler'")
    print("4. Trigger: Daily")
    print("5. Start time: [Choose your preferred start time]")
    print("6. Repeat task every: 1 hour")
    print("7. For a duration of: Indefinitely (or choose your duration)")
    print(f"8. Action: Start a program")
    print(f"9. Program/script: {os.path.abspath(bat_filename)}")
    print("10. Click Finish")
    print("=" * 50)
    print("\nüí° The crawler will run every 1 hour from when you activate it")
    print("üí° Each batch will crawl ~600 articles (60 keywords x 10 articles)")
    print("üí° To stop: Disable or delete the task in Task Scheduler")

def run_manual_test():
    """
    Ch·∫°y test th·ªß c√¥ng ƒë·ªÉ ki·ªÉm tra
    """
    print("üß™ MANUAL TEST MODE")
    print("=" * 40)
    
    # ƒê·ªçc keywords
    csv_file_path = "keyword_csvs/Keywords.csv"
    keywords = read_keywords_from_csv(csv_file_path, keyword_column="Keywords")
    
    if not keywords:
        print("‚ùå No keywords loaded. Exiting...")
        return
    
    # L·∫•y 5 keywords ƒë·∫ßu ti√™n ƒë·ªÉ test nhanh
    test_keywords = keywords[:5]
    print(f"üß™ Testing with {len(test_keywords)} keywords:")
    for i, kw in enumerate(test_keywords, 1):
        print(f"   {i}. {kw}")
    print()
    
    # Ch·∫°y batch test
    batch_file = batch_crawl_keywords(
        keywords=test_keywords,
        articles_per_keyword=10,
        batch_id=f"test_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    )
    
    print(f"\nüìÅ Test batch file: {batch_file}")
    print("‚úÖ Manual test completed!")

def main():
    """H√†m ch√≠nh ƒë∆∞·ª£c c·∫≠p nh·∫≠t ƒë·ªÉ h·ªó tr·ª£ c√°c mode kh√°c nhau"""
    
    import sys
    
    # Ki·ªÉm tra command line arguments
    if len(sys.argv) > 1:
        if sys.argv[1] == '--scheduled':
            # Mode ch·∫°y theo schedule
            scheduled_crawler()
            return
        elif sys.argv[1] == '--test':
            # Mode test th·ªß c√¥ng
            run_manual_test()
            return
        elif sys.argv[1] == '--setup':
            # Mode setup task scheduler
            create_batch_schedule_script()
            return
    
    # Mode m·∫∑c ƒë·ªãnh - h·ªèi user
    print("üî• GOOGLE NEWS BATCH CRAWLER")
    print("=" * 50)
    print("Select mode:")
    print("1. Manual test (5 keywords x 10 articles)")
    print("2. Create scheduler setup")
    print("3. Run scheduled crawl (if within working hours)")
    print("4. Full crawl (original mode)")
    
    choice = input("\nEnter your choice (1-4): ").strip()
    
    if choice == '1':
        run_manual_test()
    elif choice == '2':
        create_batch_schedule_script()
    elif choice == '3':
        scheduled_crawler()
    elif choice == '4':
        # Mode crawl ƒë·∫ßy ƒë·ªß nh∆∞ c≈©
        csv_file_path = "keyword_csvs/Keywords.csv"
        keywords = read_keywords_from_csv(csv_file_path, keyword_column="Keywords")
        
        if not keywords:
            print("‚ùå No keywords loaded from CSV. Please check your file.")
            return
        
        target_articles_per_keyword = 200
        csv_file = crawl_multiple_keywords_deep(
            keywords=keywords,
            target_articles_per_keyword=target_articles_per_keyword
        )
        
        print(f"\nüìÅ Data exported to: {csv_file}")
    else:
        print("‚ùå Invalid choice. Exiting...")

if __name__ == "__main__":
    main()