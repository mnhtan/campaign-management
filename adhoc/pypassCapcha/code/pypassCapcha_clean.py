import requests
import time
import random
from bs4 import BeautifulSoup
from urllib.parse import quote
import csv
from datetime import datetime
from dataclasses import dataclass
from typing import List

# Setup logging for simple output
import logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class NewsArticle:
    headline: str
    link: str
    date: str
    source: str = ""

@dataclass
class CrawlStats:
    total_keywords: int = 0
    total_articles: int = 0
    captcha_skipped: int = 0
    errors: int = 0
    successful_requests: int = 0
    failed_requests: int = 0

def generate_keyword_variations(base_keyword: str, max_pages: int = 10) -> List[str]:
    """
    T·∫°o c√°c variation c·ªßa keyword ƒë·ªÉ m√¥ ph·ªèng search nhi·ªÅu trang
    
    Args:
        base_keyword: Keyword g·ªëc
        max_pages: S·ªë "trang" mu·ªën search
    
    Returns:
        List c√°c keyword variations
    """
    variations = [base_keyword]  # Keyword g·ªëc
    
    # Th√™m c√°c variation ƒë·ªÉ l·∫•y nhi·ªÅu b√†i h∆°n
    suffixes = [
        "news", "latest", "updates", "trends", "developments", 
        "breakthrough", "innovation", "research", "market", "industry"
    ]
    
    time_suffixes = [
        "2024", "2025", "recent", "today", "this week", "latest news"
    ]
    
    # Th√™m suffixes
    for suffix in suffixes[:max_pages-1]:
        if len(variations) >= max_pages:
            break
        variations.append(f"{base_keyword} {suffix}")
    
    # N·∫øu v·∫´n ch∆∞a ƒë·ªß, th√™m time suffixes
    for time_suffix in time_suffixes:
        if len(variations) >= max_pages:
            break
        variations.append(f"{base_keyword} {time_suffix}")
    
    return variations[:max_pages]

def simple_crawl_rss(keyword: str, max_results: int = 20, stats: CrawlStats = None) -> List[NewsArticle]:
    """Crawl Google News RSS ƒë∆°n gi·∫£n - √≠t b·ªã ch·∫∑n"""
    if stats is None:
        stats = CrawlStats()
    
    try:
        # Google News RSS URL v·ªõi keyword
        encoded_keyword = quote(keyword)
        rss_url = f"https://news.google.com/rss/search?q={encoded_keyword}&hl=en&gl=us&ceid=US:en"
        
        # Headers ƒë∆°n gi·∫£n
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        # G·ªçi RSS
        response = requests.get(rss_url, headers=headers, timeout=30)
        response.raise_for_status()
        
        # Parse RSS v·ªõi lxml ho·∫∑c html.parser l√†m fallback
        try:
            soup = BeautifulSoup(response.content, 'xml')
        except:
            try:
                soup = BeautifulSoup(response.content, 'lxml-xml')
            except:
                soup = BeautifulSoup(response.content, 'html.parser')
        
        articles = []
        
        # T√¨m t·∫•t c·∫£ c√°c item trong RSS
        items = soup.find_all('item')
        
        for i, item in enumerate(items[:max_results]):
            try:
                title = item.title.text if item.title else "N/A"
                link = item.link.text if item.link else "N/A"
                pub_date = item.pubDate.text if item.pubDate else "N/A"
                
                # L·∫•y source t·ª´ description ho·∫∑c source tag
                source = "N/A"
                if item.source:
                    source = item.source.text
                elif item.description:
                    desc = item.description.text
                    # Extract source t·ª´ description n·∫øu c√≥
                    import re
                    source_match = re.search(r'<a[^>]*>([^<]+)</a>', desc)
                    if source_match:
                        source = source_match.group(1)
                
                article = NewsArticle(
                    headline=title,
                    link=link,
                    date=pub_date,
                    source=source
                )
                articles.append(article)
                
            except Exception as e:
                stats.errors += 1
                continue
        
        stats.successful_requests += 1
        return articles
        
    except Exception as e:
        stats.failed_requests += 1
        stats.errors += 1
        return []

def extract_domain(url: str) -> str:
    """Tr√≠ch xu·∫•t domain t·ª´ URL"""
    try:
        if not url or url == 'N/A':
            return 'N/A'
        
        from urllib.parse import urlparse
        parsed = urlparse(url)
        domain = parsed.netloc
        
        # Lo·∫°i b·ªè 'www.' n·∫øu c√≥
        if domain.startswith('www.'):
            domain = domain[4:]
            
        return domain
    except:
        return 'N/A'

def crawl_single_keyword_for_multi(keyword: str, target_articles: int, stats: CrawlStats) -> List[NewsArticle]:
    """
    Crawl 1 keyword s√¢u cho multi-keyword crawl
    
    Args:
        keyword: Keyword mu·ªën crawl
        target_articles: S·ªë b√†i m·ª•c ti√™u
        stats: Object th·ªëng k√™ chung
    
    Returns:
        List c√°c b√†i b√°o
    """
    
    all_articles = []
    seen_links = set()  # ƒê·ªÉ tr√°nh duplicate
    
    # T√≠nh s·ªë trang c·∫ßn crawl (m·ªói trang ~20 b√†i)
    articles_per_page = 20
    estimated_pages = (target_articles // articles_per_page) + 2  # +2 ƒë·ªÉ ƒë·∫£m b·∫£o ƒë·ªß
    
    print(f"üîç Crawling up to {estimated_pages} variations of '{keyword}'...")
    
    # T·∫°o c√°c variation c·ªßa keyword
    keyword_variations = generate_keyword_variations(keyword, estimated_pages)
    
    for page_num, variation in enumerate(keyword_variations, 1):
        if len(all_articles) >= target_articles:
            print(f"üéØ Reached target of {target_articles} articles, stopping...")
            break
            
        print(f"   üìÑ Page {page_num}: {variation}")
        
        # Delay gi·ªØa c√°c trang
        if page_num > 1:
            delay = random.uniform(2, 4)
            print(f"   ‚è≥ Delay: {delay:.1f}s...")
            time.sleep(delay)
        
        # Crawl t·ª´ng variation
        page_articles = simple_crawl_rss(variation, articles_per_page, stats)
        
        # L·ªçc b·ªè duplicate d·ª±a tr√™n link
        new_articles = []
        for article in page_articles:
            if article.link not in seen_links:
                seen_links.add(article.link)
                new_articles.append(article)
        
        # Th√™m v√†o danh s√°ch t·ªïng
        all_articles.extend(new_articles)
        
        print(f"   ‚úÖ Page {page_num}: +{len(new_articles)} new articles (total: {len(all_articles)})")
        
        # Progress bar cho keyword n√†y
        progress = min(len(all_articles) / target_articles * 100, 100)
        print(f"   üìä Progress: {progress:.1f}% ({len(all_articles)}/{target_articles})")
    
    print(f"üéØ Total articles for '{keyword}': {len(all_articles)}")
    return all_articles

def print_multi_keywords_stats(stats: CrawlStats, csv_filename: str, keywords: List[str], target_per_keyword: int, total_articles: int):
    """In th·ªëng k√™ cho multi-keywords crawl"""
    print("\n" + "=" * 70)
    print("üéØ MULTI-KEYWORDS CRAWL RESULTS")
    print("=" * 70)
    print(f"üìù Total Keywords: {len(keywords)}")
    print(f"üì∞ Total Articles Found: {total_articles}")
    print(f"üéØ Target Articles: {len(keywords) * target_per_keyword}")
    print(f"üìä Average Articles per Keyword: {total_articles / len(keywords):.1f}")
    print(f"‚úÖ Successful Requests: {stats.successful_requests}")
    print(f"‚ùå Failed Requests: {stats.failed_requests}")
    print(f"‚ö†Ô∏è Total Errors: {stats.errors}")
    print(f"üö´ Captcha Skipped: {stats.captcha_skipped}")
    print(f"üìÅ Output File: {csv_filename}")
    
    # Progress percentage
    target_total = len(keywords) * target_per_keyword
    progress = min(total_articles / target_total * 100, 100) if target_total > 0 else 0
    print(f"üìà Overall Achievement: {progress:.1f}%")
    
    if total_articles >= target_total:
        print("üéâ All targets achieved successfully!")
    else:
        print(f"‚ÑπÔ∏è Found {total_articles} articles (target was {target_total})")
    
    print("=" * 70)
    print("üéâ Multi-keywords crawl completed!")

def crawl_multiple_keywords_deep(keywords: List[str], target_articles_per_keyword: int = 200, csv_filename: str = None) -> str:
    """
    Crawl nhi·ªÅu keywords, m·ªói keyword l·∫•y ~200 b√†i
    
    Args:
        keywords: Danh s√°ch keywords
        target_articles_per_keyword: S·ªë b√†i m·ª•c ti√™u cho m·ªói keyword
        csv_filename: T√™n file CSV output
    
    Returns:
        T√™n file CSV ƒë√£ t·∫°o
    """
    
    # T·∫°o t√™n file CSV v·ªõi timestamp
    if csv_filename is None:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        csv_filename = f"multi_keywords_crawl_{timestamp}.csv"
    
    # Kh·ªüi t·∫°o stats t·ªïng
    total_stats = CrawlStats()
    total_stats.total_keywords = len(keywords)
    
    print(f"üî• MULTI-KEYWORDS DEEP CRAWLER")
    print("=" * 70)
    print(f"üìù Keywords: {len(keywords)}")
    print(f"üìä Target articles per keyword: {target_articles_per_keyword}")
    print(f"üéØ Total target articles: {len(keywords) * target_articles_per_keyword}")
    print(f"üìÅ Output file: {csv_filename}")
    print("=" * 70)
    
    # Hi·ªÉn th·ªã keywords s·∫Ω crawl
    print("üìã KEYWORDS TO CRAWL:")
    for i, keyword in enumerate(keywords, 1):
        print(f"   {i}. {keyword}")
    print()
    
    all_articles = []
    
    # T·∫°o v√† m·ªü file CSV
    with open(csv_filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
        fieldnames = [
            'keyword', 'article_number', 'headline', 'link', 
            'date', 'source', 'domain', 'crawl_timestamp'
        ]
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        
        # Crawl t·ª´ng keyword
        for keyword_index, keyword in enumerate(keywords, 1):
            print(f"\nüîç PROCESSING KEYWORD {keyword_index}/{len(keywords)}: {keyword}")
            print("-" * 50)
            
            # Delay gi·ªØa c√°c keywords
            if keyword_index > 1:
                delay = random.uniform(8, 15)
                print(f"‚è≥ Waiting {delay:.1f} seconds between keywords...")
                time.sleep(delay)
            
            # Crawl keyword n√†y
            keyword_articles = crawl_single_keyword_for_multi(
                keyword, 
                target_articles_per_keyword, 
                total_stats
            )
            
            # Ghi t·∫•t c·∫£ b√†i b√°o c·ªßa keyword n√†y v√†o CSV
            for i, article in enumerate(keyword_articles, 1):
                row = {
                    'keyword': keyword,
                    'article_number': i,
                    'headline': article.headline,
                    'link': article.link,
                    'date': article.date,
                    'source': article.source,
                    'domain': extract_domain(article.link),
                    'crawl_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }
                writer.writerow(row)
            
            all_articles.extend(keyword_articles)
            
            # Progress update
            print(f"‚úÖ Completed {keyword}: {len(keyword_articles)} articles")
            print(f"üìä Overall progress: {keyword_index}/{len(keywords)} keywords, {len(all_articles)} total articles")
    
    total_stats.total_articles = len(all_articles)
    
    # In th·ªëng k√™ cu·ªëi c√πng
    print_multi_keywords_stats(total_stats, csv_filename, keywords, target_articles_per_keyword, len(all_articles))
    
    return csv_filename

def read_keywords_from_csv(csv_file_path: str, keyword_column: str = None, skip_header: bool = True) -> List[str]:
    """
    ƒê·ªçc keywords t·ª´ file CSV
    
    Args:
        csv_file_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file CSV ch·ª©a keywords
        keyword_column: T√™n c·ªôt ch·ª©a keywords (n·∫øu None s·∫Ω l·∫•y c·ªôt ƒë·∫ßu ti√™n)
        skip_header: C√≥ b·ªè qua header row kh√¥ng
    
    Returns:
        List c√°c keywords t·ª´ CSV
    """
    keywords = []
    
    try:
        print(f"üìñ Reading keywords from CSV file: {csv_file_path}")
        
        with open(csv_file_path, 'r', encoding='utf-8-sig') as csvfile:
            # Detect delimiter
            sample = csvfile.read(1024)
            csvfile.seek(0)
            
            delimiter = ','
            if ';' in sample and sample.count(';') > sample.count(','):
                delimiter = ';'
            elif '\t' in sample:
                delimiter = '\t'
            
            print(f"üìä Detected CSV delimiter: '{delimiter}'")
            
            reader = csv.reader(csvfile, delimiter=delimiter)
            
            # Skip header if needed
            if skip_header:
                try:
                    header = next(reader)
                    print(f"üìã CSV Header: {header}")
                    
                    # T·ª± ƒë·ªông detect keyword column n·∫øu kh√¥ng ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh
                    if keyword_column is None:
                        keyword_column = header[0]  # L·∫•y c·ªôt ƒë·∫ßu ti√™n
                        keyword_col_index = 0
                    else:
                        # T√¨m index c·ªßa keyword column
                        keyword_col_index = None
                        for i, col in enumerate(header):
                            if col.lower().strip() == keyword_column.lower().strip():
                                keyword_col_index = i
                                break
                        
                        if keyword_col_index is None:
                            print(f"‚ö†Ô∏è Column '{keyword_column}' not found. Using first column.")
                            keyword_col_index = 0
                    
                    print(f"üéØ Using column '{header[keyword_col_index]}' (index: {keyword_col_index}) for keywords")
                    
                except StopIteration:
                    print("‚ö†Ô∏è CSV file is empty")
                    return []
            else:
                keyword_col_index = 0
            
            # ƒê·ªçc keywords
            for row_num, row in enumerate(reader, start=2 if skip_header else 1):
                if row and len(row) > keyword_col_index:
                    keyword = row[keyword_col_index].strip()
                    if keyword:  # B·ªè qua d√≤ng tr·ªëng
                        keywords.append(keyword)
                        print(f"   ‚úÖ Row {row_num}: '{keyword}'")
                else:
                    print(f"   ‚ö†Ô∏è Row {row_num}: Empty or invalid row")
        
        print(f"üìä Total keywords loaded: {len(keywords)}")
        return keywords
        
    except FileNotFoundError:
        print(f"‚ùå File not found: {csv_file_path}")
        return []
    except Exception as e:
        print(f"‚ùå Error reading CSV file: {str(e)}")
        return []

def main():
    """H√†m ch√≠nh ƒë·ªÉ crawl keywords t·ª´ file Keywords.csv"""
    
    print("üî• GOOGLE NEWS KEYWORD CRAWLER (CLEANED VERSION)")
    print("=" * 60)
    
    # =================================================================
    # üìù ƒê·ªåC KEYWORDS T·ª™ FILE CSV:
    # =================================================================
    csv_file_path = "keyword_csvs/Keywords.csv"  # ƒê∆∞·ªùng d·∫´n file CSV
    
    print(f"üìñ Reading keywords from: {csv_file_path}")
    
    # ƒê·ªçc keywords t·ª´ CSV
    keywords = read_keywords_from_csv(csv_file_path, keyword_column="Keywords")
    
    if not keywords:
        print("‚ùå No keywords loaded from CSV. Please check your file.")
        return
    
    # =================================================================
    # ‚öôÔ∏è C·∫§U H√åNH CRAWL:
    # =================================================================
    target_articles_per_keyword = 200   # S·ªë b√†i m·ªói keyword
    
    print("\n‚öôÔ∏è CRAWL CONFIGURATION:")
    print(f"   üìù Number of keywords: {len(keywords)}")
    print(f"   üìä Target articles per keyword: {target_articles_per_keyword}")
    print(f"   üéØ Total target articles: {len(keywords) * target_articles_per_keyword}")
    print()
    
    # Hi·ªÉn th·ªã m·ªôt s·ªë keywords ƒë·∫ßu ti√™n
    print("üìã FIRST 10 KEYWORDS TO CRAWL:")
    for i, keyword in enumerate(keywords[:10], 1):
        print(f"   {i}. {keyword}")
    
    if len(keywords) > 10:
        print(f"   ... and {len(keywords) - 10} more keywords")
    print()
    
    # B·∫Øt ƒë·∫ßu crawl
    print("üöÄ Starting crawl...")
    csv_file = crawl_multiple_keywords_deep(
        keywords=keywords,
        target_articles_per_keyword=target_articles_per_keyword
    )
    
    print(f"\nüìÅ Data exported to: {csv_file}")
    print(f"üîç You now have data for {len(keywords)} keywords!")
    print("üí° Open the CSV file in Excel to view your data!")

if __name__ == "__main__":
    main() 